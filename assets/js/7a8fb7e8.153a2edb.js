"use strict";(self.webpackChunknoodles=self.webpackChunknoodles||[]).push([[5259],{8358:e=>{e.exports=JSON.parse('{"permalink":"/2011/02/15/deny-access-to-website-but-allow-robots-txt","source":"@site/blog/2011-02-15-deny-access-to-website-but-allow-robots-txt.mdx","title":"Deny access to website, but allow robots.txt","description":"I had a problem where Googlebot was indexing a development site, so we locked it down using apache basic http auth. Now Googlebot was being served with a 401 when accessing the site, but because it had no stored robots.txt it was persistently trying to crawl the site.","date":"2011-02-15T00:00:00.000Z","tags":[],"readingTime":0.53,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Deny access to website, but allow robots.txt"},"unlisted":false,"prevItem":{"title":"Change CVS path","permalink":"/2011/02/16/change-cvs-path"},"nextItem":{"title":"unzip 6 for RHEL 5.6","permalink":"/2011/02/15/unzip-6-for-rhel-5-6"}}')},8453:(e,t,s)=>{s.d(t,{R:()=>i,x:()=>r});var n=s(6540);const o={},a=n.createContext(o);function i(e){const t=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),n.createElement(a.Provider,{value:t},e.children)}},9397:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>n,toc:()=>l});var n=s(8358),o=s(4848),a=s(8453);const i={title:"Deny access to website, but allow robots.txt"},r=void 0,c={authorsImageUrls:[]},l=[];function h(e){const t={code:"code",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:"I had a problem where Googlebot was indexing a development site, so we locked it down using apache basic http auth. Now Googlebot was being served with a 401 when accessing the site, but because it had no stored robots.txt it was persistently trying to crawl the site."}),"\n",(0,o.jsx)(t.p,{children:"Using the following allows anyone to access robots.txt but denies access to the rest of the site:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'<Directory "/home/username/www">\nAuthUserFile /home/username/.htpasswd\nAuthName "Client Access"\nAuthType Basic\nrequire valid-use  \n\n<Files "robots.txt">\nAuthType Basic\nsatisfy any\n</Files>\n</Directory>\n'})}),"\n",(0,o.jsx)(t.p,{children:"Eventually Googlebot will get the hint and stop indexing the site and we can remove existing content using webmaster tools."})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}}}]);