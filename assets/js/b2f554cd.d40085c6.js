"use strict";(self.webpackChunknoodles=self.webpackChunknoodles||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2020/01/07/yarn-audit-fix","metadata":{"permalink":"/blog/2020/01/07/yarn-audit-fix","source":"@site/blog/2020-01-07-yarn-audit-fix.mdx","title":"Yarn audit fix","description":"Yarn doesn\'t have the ability to fix the problems it finds in a security audit (like npm does). There is a workaround that I found on a github thread though: It\'s not pretty but it does the job.","date":"2020-01-07T00:00:00.000Z","formattedDate":"January 7, 2020","tags":[],"readingTime":0.48,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Yarn audit fix"},"unlisted":false,"nextItem":{"title":"Fixing Twig deprecations in Symfony 4.4","permalink":"/blog/fixing-twig-deprecations-in-symfony-4-4"}},"content":"<p>Yarn doesn\'t have the ability to fix the problems it finds in a security audit (like npm does). There is a workaround that I found on a github thread though: It\'s not pretty but it does the job.</p>\\n\\n\\n\x3c!--truncate--\x3e\\n\\nYarn doesn\'t have the ability to fix the problems it finds in a security audit (like npm does). There is a workaround that I found on a github thread though:  \\n\\n```\\nnpm install\\nnpm audit fix --force # breaking changes\\nrm yarn.lock\\nyarn import\\nyarn audit\\nrm package-lock.json\\n```\\n\\nIt\'s not pretty but it does the job."},{"id":"fixing-twig-deprecations-in-symfony-4-4","metadata":{"permalink":"/blog/fixing-twig-deprecations-in-symfony-4-4","source":"@site/blog/2019-11-27-fixing-twig-deprecations-in-symfony-4-4.mdx","title":"Fixing Twig deprecations in Symfony 4.4","description":"I recently update to Symfony 4.4 and had to work through a few deprecations. Some were straight forward, some were not. This Twig one was not [&hellip;]","date":"2019-11-27T00:00:00.000Z","formattedDate":"November 27, 2019","tags":[],"readingTime":0.445,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"fixing-twig-deprecations-in-symfony-4-4","title":"Fixing Twig deprecations in Symfony 4.4"},"unlisted":false,"prevItem":{"title":"Yarn audit fix","permalink":"/blog/2020/01/07/yarn-audit-fix"},"nextItem":{"title":"Running Akamai Sandbox in Docker with HTTPS","permalink":"/blog/running-akamai-sandbox-in-docker-with-https"}},"content":"<p>I recently update to Symfony 4.4 and had to work through a few deprecations. Some were straight forward, some were not. This Twig one was not [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\n\\nI recently update to Symfony 4.4 and had to work through a few deprecations. Some were straight forward, some were not. This Twig one was not:  \\n  \\n```\\nThe \\"twig.exception_controller\\" configuration key has been deprecated in Symfony 4.4, set it to \\"null\\" and use \\"framework.error_controller\\" configuration key instead.\\n```\\n  \\nThis was resolved by adding the following to config/packages/twig.yaml:  \\n\\n```  \\nexception_controller: null\\n```"},{"id":"running-akamai-sandbox-in-docker-with-https","metadata":{"permalink":"/blog/running-akamai-sandbox-in-docker-with-https","source":"@site/blog/2018-10-12-running-akamai-sandbox-in-docker-with-https.mdx","title":"Running Akamai Sandbox in Docker with HTTPS","description":"Akamai\'s new Sandbox can be run on local development environments, so you can test changes in development with production like CDN settings. This allows you to more quickly identify issues before rolling out to production. The Akamai sandbox (or DevPoPs) is a Java app (see https://bit.ly/aka-sb-gh). This Java app can be containerised for portability/ease of [&hellip;]","date":"2018-10-12T00:00:00.000Z","formattedDate":"October 12, 2018","tags":[],"readingTime":2.515,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"running-akamai-sandbox-in-docker-with-https","title":"Running Akamai Sandbox in Docker with HTTPS"},"unlisted":false,"prevItem":{"title":"Fixing Twig deprecations in Symfony 4.4","permalink":"/blog/fixing-twig-deprecations-in-symfony-4-4"},"nextItem":{"title":"ipv6 workaround for Unifi USG on 2 Degrees UFB","permalink":"/blog/ipv6-workaround-for-unifi-usg-on-2-degrees-ufb"}},"content":"<p>Akamai\'s new Sandbox can be run on local development environments, so you can test changes in development with production like CDN settings. This allows you to more quickly identify issues before rolling out to production. The Akamai sandbox (or DevPoPs) is a Java app (see https://bit.ly/aka-sb-gh). This Java app can be containerised for portability/ease of [&hellip;]</p>\\r\\n\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\nAkamai\'s new Sandbox can be run on local development environments, so you can test changes in development with production like CDN settings. This allows you to more quickly identify issues before rolling out to production.  \\r\\n\\r\\nThe Akamai sandbox (or DevPoPs) is a Java app (see <a href=\\"https://bit.ly/aka-sb-gh\\" rel=\\"noopener\\" target=\\"_blank\\">https://bit.ly/aka-sb-gh</a>). This Java app can be containerised for portability/ease of setup and use.  \\r\\n\\r\\nI created a simple docker compose setup (<a href=\\"https://github.com/NoodlesNZ/devpops-test\\" rel=\\"noopener\\" target=\\"_blank\\">https://github.com/NoodlesNZ/devpops-test</a>).  \\r\\n\\r\\nThis can be used with a real certificate (which is signed by a CA), but works as well with a self signed certificate using (on a Mac):  \\r\\n\\r\\n```\\r\\nopenssl req \\\\\\r\\n    -newkey rsa:2048 \\\\\\r\\n    -x509 \\\\\\r\\n    -nodes \\\\\\r\\n    -keyout server.key \\\\\\r\\n    -new \\\\\\r\\n    -out server.crt \\\\\\r\\n    -subj /CN=www.example.com \\\\\\r\\n    -reqexts SAN \\\\\\r\\n    -extensions SAN \\\\\\r\\n    -config <(cat /System/Library/OpenSSL/openssl.cnf \\\\\\r\\n        <(printf \'[SAN]\\\\nsubjectAltName=DNS:www.example.com\')) \\\\\\r\\n    -sha256 \\\\\\r\\n    -days 3650\\r\\n```\\r\\nThis is included in the conf/config.json file:  \\r\\n\\r\\n```\\r\\n{\\r\\n  \\"connectorServerInfo\\": {\\r\\n    \\"secure\\": true,\\r\\n    \\"port\\": 443,\\r\\n    \\"host\\": \\"0.0.0.0\\",\\r\\n    \\"cert\\": {\\r\\n      \\"certChainPath\\": \\"./conf/server.crt\\",\\r\\n      \\"keyPath\\": \\"./conf/server.key\\"\\r\\n    }\\r\\n  },\\r\\n  \\"originMappings\\": [\\r\\n    {\\r\\n      \\"from\\": \\"<ORIGIN HOST>\\",\\r\\n      \\"to\\": {\\r\\n        \\"secure\\": true,\\r\\n        \\"port\\": 8443,\\r\\n        \\"host\\": \\"host.docker.internal\\"\\r\\n      }\\r\\n    }\\r\\n  ],\\r\\n  \\"jwt\\": \\"<ENTER JWT HERE>\\"\\r\\n}\\r\\n```\\r\\nExplaining a few options in the config.json file.  \\r\\n\\r\\nIn the connectorServerInfo section:\\r\\n- secure: true - enables https\\r\\n- port: 443 - listens on port 443\\r\\n- host: 0.0.0.0 - bind to all ip addresses (needed for docker as binding to 127.0.0.1 doesn\'t work)\\r\\n- cert - public/private key as generated with openssl  \\r\\n\\r\\nIn the originMappings section:\\r\\n- from: `<ORIGIN HOST>` - the origin hostname in your Akamai property, e.g. origin.example.com\\r\\n- to - the local/development origin\\r\\n- secure: true - enabled https on the new origin\\r\\n- port: 8443 - As the Sandbox is now listening on port 443, the origin needs to be on a different port\\r\\n- host: host.docker.internal - special docker hostname on mac, which resolves to the host\'s ip address. This assumes that your dev server is also hosted on your mac.  \\r\\n\\r\\nThis setup can also be incorporated into an existing docker compose setup, e.g.  \\r\\n\\r\\n```\\r\\nversion: \'2\'\\r\\nservices:\\r\\n  web:\\r\\n    image: example/web:latest\\r\\n    networks:\\r\\n      - appnet\\r\\n  devpops:\\r\\n    image: noodlesnz/devpops:latest\\r\\n    volumes:\\r\\n      - ./conf:/opt/devpops/conf\\r\\n    ports:\\r\\n      - 443:443\\r\\n    networks:\\r\\n      - appnet\\r\\nnetworks:\\r\\n  appnet:\\r\\n    driver: \\"bridge\\"\\r\\n```\\r\\nWith web and devpops sharing the same docker network, you can use the host \\"web\\" with your config.json, e.g.  \\r\\n\\r\\n```\\r\\n{\\r\\n  \\"connectorServerInfo\\": {\\r\\n    \\"secure\\": true,\\r\\n    \\"port\\": 443,\\r\\n    \\"host\\": \\"0.0.0.0\\",\\r\\n    \\"cert\\": {\\r\\n      \\"certChainPath\\": \\"./conf/server.crt\\",\\r\\n      \\"keyPath\\": \\"./conf/server.key\\"\\r\\n    }\\r\\n  },\\r\\n  \\"originMappings\\": [\\r\\n    {\\r\\n      \\"from\\": \\"<ORIGIN HOST>\\",\\r\\n      \\"to\\": {\\r\\n        \\"secure\\": true,\\r\\n        \\"port\\": 443,\\r\\n        \\"host\\": \\"web\\"\\r\\n      }\\r\\n    }\\r\\n  ],\\r\\n  \\"jwt\\": \\"<ENTER JWT HERE>\\"\\r\\n}\\r\\n```\\r\\nThis also means that the development origin can only be accessed through the Akamai Sandbox, as web doesn\'t have any ports exposed."},{"id":"ipv6-workaround-for-unifi-usg-on-2-degrees-ufb","metadata":{"permalink":"/blog/ipv6-workaround-for-unifi-usg-on-2-degrees-ufb","source":"@site/blog/2018-09-20-ipv6-workaround-for-unifi-usg-on-2-degrees-ufb.mdx","title":"ipv6 workaround for Unifi USG on 2 Degrees UFB","description":"I had an issue where our USG Pro was not getting ipv6 from 2 Degrees UFB after upgrading our Controller to 5.8. After a lot of messing around with this, I found a workaround originally posted here//community.ubnt.com/t5/UniFi-Routing-Switching/USG-DHCPv6-PD-bug-when-using-PPPoE/td-p/2487710. Digging into the dhcpv6-pd logs shows this: Dumping out the config I saw that I had two [&hellip;]","date":"2018-09-20T00:00:00.000Z","formattedDate":"September 20, 2018","tags":[],"readingTime":1.17,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ipv6-workaround-for-unifi-usg-on-2-degrees-ufb","title":"ipv6 workaround for Unifi USG on 2 Degrees UFB"},"unlisted":false,"prevItem":{"title":"Running Akamai Sandbox in Docker with HTTPS","permalink":"/blog/running-akamai-sandbox-in-docker-with-https"},"nextItem":{"title":"Speed up Jenkins phpcs (PHP CodeSniffer)","permalink":"/blog/speed-up-jenkins-phpcs-php-codesniffer"}},"content":"<p>I had an issue where our USG Pro was not getting ipv6 from 2 Degrees UFB after upgrading our Controller to 5.8. After a lot of messing around with this, I found a workaround originally posted here: https://community.ubnt.com/t5/UniFi-Routing-Switching/USG-DHCPv6-PD-bug-when-using-PPPoE/td-p/2487710. Digging into the dhcpv6-pd logs shows this: Dumping out the config I saw that I had two [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI had an issue where our USG Pro was not getting ipv6 from 2 Degrees UFB after upgrading our Controller to 5.8. After a lot of messing around with this, I found a workaround originally posted here: <a href=\\"https://community.ubnt.com/t5/UniFi-Routing-Switching/USG-DHCPv6-PD-bug-when-using-PPPoE/td-p/2487710\\" target=\\"_blank\\" rel=\\"noopener\\">https://community.ubnt.com/t5/UniFi-Routing-Switching/USG-DHCPv6-PD-bug-when-using-PPPoE/td-p/2487710</a>.  \\n\\nDigging into the dhcpv6-pd logs shows this:  \\n\\n<script src=\\"https://gist.github.com/NoodlesNZ/f8ba0f83c0210e75a5587da9d4e2a606.js?file=gistfile1.txt\\"><\/script>  \\n\\nDumping out the config I saw that I had two dhcpv6-pd blocks, one under interface eth2 vif 10 and the other under interface eth2 vif 10 pppoe 2 (where it should be):  \\n\\n<script src=\\"https://gist.github.com/NoodlesNZ/f8ba0f83c0210e75a5587da9d4e2a606.js?file=gistfile2.txt\\"><\/script>  \\n\\nIt was possible to temporarily fix this issue by removing the first block:  \\n\\n<script src=\\"https://gist.github.com/NoodlesNZ/f8ba0f83c0210e75a5587da9d4e2a606.js?file=gistfile3.txt\\"><\/script>  \\n\\nThis allowed our USG to get ipv6 from our ISP and now all the clients on the network also got ipv6.  \\n\\nTo make this more permanent I had to add this file on the USG under /config/scripts/post-config.d/dhcp.sh  \\n\\n<script src=\\"https://gist.github.com/NoodlesNZ/f8ba0f83c0210e75a5587da9d4e2a606.js?file=dhcp.sh\\"><\/script>  \\n\\nTo run this script, I added the following on the Controller in /usr/lib/unifi/data/sites/default/config.gateway.json:  \\n\\n<script src=\\"https://gist.github.com/NoodlesNZ/f8ba0f83c0210e75a5587da9d4e2a606.js?file=config.gateway.json\\"><\/script>  \\n\\nThis runs the dhcp.sh script 2 minutes after provisioning and then the script removes the scheduled task (as it only needs to run once)."},{"id":"speed-up-jenkins-phpcs-php-codesniffer","metadata":{"permalink":"/blog/speed-up-jenkins-phpcs-php-codesniffer","source":"@site/blog/2015-07-08-speed-up-jenkins-phpcs-php-codesniffer.mdx","title":"Speed up Jenkins phpcs (PHP CodeSniffer)","description":"PHP CodeSniffer in our Jenkins CI was always one of the slowest tasks as it ran across our whole code base. LB Denker from Etsy wrote a good piece of software called CSRunner which looked to solve this problem by only running phpcs on files that had changed in the last 7 days (or so). [&hellip;]","date":"2015-07-08T00:00:00.000Z","formattedDate":"July 8, 2015","tags":[],"readingTime":1.075,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"speed-up-jenkins-phpcs-php-codesniffer","title":"Speed up Jenkins phpcs (PHP CodeSniffer)"},"unlisted":false,"prevItem":{"title":"ipv6 workaround for Unifi USG on 2 Degrees UFB","permalink":"/blog/ipv6-workaround-for-unifi-usg-on-2-degrees-ufb"},"nextItem":{"title":"Show MySQL engine tablespace size","permalink":"/blog/show-mysql-engine-tablespace-size"}},"content":"<p>PHP CodeSniffer in our Jenkins CI was always one of the slowest tasks as it ran across our whole code base. LB Denker from Etsy wrote a good piece of software called CSRunner which looked to solve this problem by only running phpcs on files that had changed in the last 7 days (or so). [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nPHP CodeSniffer in our Jenkins CI was always one of the slowest tasks as it ran across our whole code base. LB Denker from Etsy wrote a good piece of software called <a href=\\"https://github.com/elblinkin/CSRunner\\" target=\\"_blank\\">CSRunner</a> which looked to solve this problem by only running phpcs on files that had changed in the last 7 days (or so). It is written as a PHP script that was run from Jenkins.  \\n\\nI took this idea and adapted it to run in Ant. Instead of looking at files changed in x days, it looks at the checkstyle report from the last run and gets a list of files with problems. It merges this with any files that have changed since the last build. In theory it should bring the run time down (assuming you have a low number of files with problems).  \\n\\n<script src=\\"https://gist.github.com/NoodlesNZ/ea5ab91c03f4dfb13ca0.js\\"><\/script>  \\n\\nI\'m open to any ideas on how to improve this as I\'m not that experienced with Ant."},{"id":"show-mysql-engine-tablespace-size","metadata":{"permalink":"/blog/show-mysql-engine-tablespace-size","source":"@site/blog/2014-10-22-show-mysql-engine-tablespace-size.mdx","title":"Show MySQL engine tablespace size","description":"I have been trying to migrate everything in MySQL to use INNODB (death to all MyISAM), but was unsure of how much data was being stored in each storage engine. You can use the following query to give a total usage for all engines: SELECT ENGINE, CONCAT(FORMAT(RIBPS/POWER(1024,pw),2),SUBSTR(&#8216; KMGT\',pw+1,1)) Usage FROM ( SELECT ENGINE,RIBPS,FLOOR(LOG(RIBPS)/LOG(1024)) pw FROM [&hellip;]","date":"2014-10-22T00:00:00.000Z","formattedDate":"October 22, 2014","tags":[],"readingTime":0.74,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"show-mysql-engine-tablespace-size","title":"Show MySQL engine tablespace size"},"unlisted":false,"prevItem":{"title":"Speed up Jenkins phpcs (PHP CodeSniffer)","permalink":"/blog/speed-up-jenkins-phpcs-php-codesniffer"},"nextItem":{"title":"Speeding up percona xtrabackup restores","permalink":"/blog/speeding-up-percona-xtrabackup-restores"}},"content":"<p>I have been trying to migrate everything in MySQL to use INNODB (death to all MyISAM), but was unsure of how much data was being stored in each storage engine. You can use the following query to give a total usage for all engines: SELECT ENGINE, CONCAT(FORMAT(RIBPS/POWER(1024,pw),2),SUBSTR(&#8216; KMGT\',pw+1,1)) `Usage` FROM ( SELECT ENGINE,RIBPS,FLOOR(LOG(RIBPS)/LOG(1024)) pw FROM [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI have been trying to migrate everything in MySQL to use INNODB (death to all MyISAM), but was unsure of how much data was being stored in each storage engine. You can use the following query to give a total usage for all engines:  \\n\\n```\\nSELECT ENGINE, CONCAT(FORMAT(RIBPS/POWER(1024,pw),2),SUBSTR(\' KMGT\',pw+1,1)) `Usage` FROM\\n(\\n    SELECT ENGINE,RIBPS,FLOOR(LOG(RIBPS)/LOG(1024)) pw\\n    FROM\\n    (\\n        SELECT ENGINE, SUM(data_length+index_length) RIBPS\\n\\t\\tFROM information_schema.tables AAA\\n\\t\\tGROUP BY ENGINE\\n\\t\\tHAVING RIBPS != 0\\n    ) AA\\n) A;\\n```\\nNow I have that information I can adjust my INNODB buffers and reduce MyISAM caches"},{"id":"speeding-up-percona-xtrabackup-restores","metadata":{"permalink":"/blog/speeding-up-percona-xtrabackup-restores","source":"@site/blog/2014-10-10-speeding-up-percona-xtrabackup-restores.mdx","title":"Speeding up percona xtrabackup restores","description":"I started playing around with using xtrabackup (or more specifically innobackupex) to backup MySQL. Most of our tables are now innodb so it didn\'t make sense to keep dumping everything out via mysqldump. I had a clone of our master db server in our virtual environment that I was trying to restore the backup onto, [&hellip;]","date":"2014-10-10T00:00:00.000Z","formattedDate":"October 10, 2014","tags":[],"readingTime":1.05,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"speeding-up-percona-xtrabackup-restores","title":"Speeding up percona xtrabackup restores"},"unlisted":false,"prevItem":{"title":"Show MySQL engine tablespace size","permalink":"/blog/show-mysql-engine-tablespace-size"},"nextItem":{"title":"SSL by default","permalink":"/blog/ssl-by-default"}},"content":"<p>I started playing around with using xtrabackup (or more specifically innobackupex) to backup MySQL. Most of our tables are now innodb so it didn\'t make sense to keep dumping everything out via mysqldump. I had a clone of our master db server in our virtual environment that I was trying to restore the backup onto, [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI started playing around with using xtrabackup (or more specifically innobackupex) to backup MySQL. Most of our tables are now innodb so it didn\'t make sense to keep dumping everything out via mysqldump.  \\n\\nI had a clone of our master db server in our virtual environment that I was trying to restore the backup onto, but it was taking hours (using <em>innobackupex &#8211;copy-back /backup/</em>). I figured that the IO on my virtual servers was just crap and I\'d have to grin and bear it. There doesn\'t seem to be much around about restoring using innobackupex, even the command options are limited for restores so I thought <em>&#8211;copy-back</em> was the only way.  \\n\\nIt seems that if your backup is on the same filesystem as where it\'s going to end up then it\'s a lot faster to use the <em>&#8211;move-back</em> option. This changed my restore time from hours to seconds.  \\n\\ne.g.<br />\\n<em>innobackupex &#8211;move-back /backup/</em>"},{"id":"ssl-by-default","metadata":{"permalink":"/blog/ssl-by-default","source":"@site/blog/2014-08-15-ssl-by-default.mdx","title":"SSL by default","description":"All the cool kids are doing it, so I\'m playing around with enabling SSL by default with HSTS. Thanks to CloudFlare and StartSSL it\'s been mostly without a hiccup.","date":"2014-08-15T00:00:00.000Z","formattedDate":"August 15, 2014","tags":[],"readingTime":0.295,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ssl-by-default","title":"SSL by default"},"unlisted":false,"prevItem":{"title":"Speeding up percona xtrabackup restores","permalink":"/blog/speeding-up-percona-xtrabackup-restores"},"nextItem":{"title":"Validator.nu RPM","permalink":"/blog/validator-nu-rpm"}},"content":"<p>All the cool kids are doing it, so I\'m playing around with enabling SSL by default with HSTS. Thanks to CloudFlare and StartSSL it\'s been mostly without a hiccup.</p>\\n\\n\\n\x3c!--truncate--\x3e\\nAll the cool kids are doing it, so I\'m playing around with enabling SSL by default with HSTS. Thanks to CloudFlare and StartSSL it\'s been mostly without a hiccup."},{"id":"validator-nu-rpm","metadata":{"permalink":"/blog/validator-nu-rpm","source":"@site/blog/2014-03-10-validator-nu-rpm.mdx","title":"Validator.nu RPM","description":"I\'ve been playing around with validator.nu the last few days. I have been trying to get a standalone version working so I could package it up and puppetize it. Unfortunately a lot of the standalone jar builders failed (java hell). I finally found that it\'s been released here//github.com/validator/validator.github.io I whipped up a basic rpm [&hellip;]","date":"2014-03-10T00:00:00.000Z","formattedDate":"March 10, 2014","tags":[],"readingTime":0.62,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"validator-nu-rpm","title":"Validator.nu RPM"},"unlisted":false,"prevItem":{"title":"SSL by default","permalink":"/blog/ssl-by-default"},"nextItem":{"title":"IPv6 in NZ","permalink":"/blog/ipv6-in-nz"}},"content":"<p>I\'ve been playing around with validator.nu the last few days. I have been trying to get a standalone version working so I could package it up and puppetize it. Unfortunately a lot of the standalone jar builders failed (java hell). I finally found that it\'s been released here: https://github.com/validator/validator.github.io I whipped up a basic rpm [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI\'ve been playing around with validator.nu the last few days. I have been trying to get a standalone version working so I could package it up and puppetize it. Unfortunately a lot of the standalone jar builders failed (java hell).  \\n\\nI finally found that it\'s been released here: <a href=\\"https://github.com/validator/validator.github.io\\">https://github.com/validator/validator.github.io</a>  \\n\\nI whipped up a basic rpm to use this and install an init script etc: <a href=\\"https://github.com/NoodlesNZ/validator-nu-rpm\\">https://github.com/NoodlesNZ/validator-nu-rpm</a>"},{"id":"ipv6-in-nz","metadata":{"permalink":"/blog/ipv6-in-nz","source":"@site/blog/2014-01-06-ipv6-in-nz.mdx","title":"IPv6 in NZ","description":"I just a quick survey of the top 500 sites in NZ (based on Alexa data) and I was disappointed to see that only two NZ based sites (excluding Google, Microsoft, Facebook etc) supported IPv6, geekzone.co.nz and nzsale.co.nz (Geekzone implemented its IPv6 via Cloudflare and NZ Sale through Akamai). Come on people, it\'s 2014. There\'s [&hellip;]","date":"2014-01-06T00:00:00.000Z","formattedDate":"January 6, 2014","tags":[],"readingTime":0.905,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ipv6-in-nz","title":"IPv6 in NZ"},"unlisted":false,"prevItem":{"title":"Validator.nu RPM","permalink":"/blog/validator-nu-rpm"},"nextItem":{"title":"Another day, another vBulletin code error","permalink":"/blog/another-day-another-vbulletin-code-error"}},"content":"<p>I just a quick survey of the top 500 sites in NZ (based on Alexa data) and I was disappointed to see that only two NZ based sites (excluding Google, Microsoft, Facebook etc) supported IPv6, geekzone.co.nz and nzsale.co.nz (Geekzone implemented its IPv6 via Cloudflare and NZ Sale through Akamai). Come on people, it\'s 2014. There\'s [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI just a quick survey of the top 500 sites in NZ (based on Alexa data) and I was disappointed to see that only two NZ based sites (excluding Google, Microsoft, Facebook etc) supported IPv6, <a href=\\"http://www.geekzone.co.nz\\" target=\\"_blank\\">geekzone.co.nz</a> and <a href=\\"http://www.nzsale.co.nz\\" target=\\"_blank\\">nzsale.co.nz (</a>Geekzone implemented its IPv6 via Cloudflare and NZ Sale through Akamai).  \\n\\nCome on people, it\'s 2014. There\'s no excuse not to support IPv6, especially with two RIRs on their last /8 and APNIC with ~13.5 million addresses remaining. What\'s really worrying is that some of the major ISPS (Telecom, Vodafone, Orcon) don\'t even have IPv6 on their public facing websites. I\'d guess that their residential customers won\'t be seeing IPv6 on their connections anytime soon and that CGN is a real possibility."},{"id":"another-day-another-vbulletin-code-error","metadata":{"permalink":"/blog/another-day-another-vbulletin-code-error","source":"@site/blog/2013-12-16-another-day-another-vbulletin-code-error.mdx","title":"Another day, another vBulletin code error","description":"It seems that vBulletin doesn\'t test on PHP 5.4 or 5.5 these days. Either that or they\'re happy to just suppress errors rather than actually fix them. I upgraded my forum today to vBulletin 4.2.2 and noticed these errors on a search page Declaration of vBForumItemSocialGroupMessage::getLoadQuery($requiredquery = \\", $force_rebuild [&hellip;]","date":"2013-12-16T00:00:00.000Z","formattedDate":"December 16, 2013","tags":[],"readingTime":0.86,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"another-day-another-vbulletin-code-error","title":"Another day, another vBulletin code error"},"unlisted":false,"prevItem":{"title":"IPv6 in NZ","permalink":"/blog/ipv6-in-nz"},"nextItem":{"title":"GPT Passback tags and validation errors","permalink":"/blog/gpt-passback-tags-and-validation-errors"}},"content":"<p>It seems that vBulletin doesn\'t test on PHP 5.4 or 5.5 these days. Either that or they\'re happy to just suppress errors rather than actually fix them. I upgraded my forum today to vBulletin 4.2.2 and noticed these errors on a search page: Warning: Declaration of vBForum_Item_SocialGroupMessage::getLoadQuery() should be compatible with vB_Model::getLoadQuery($required_query = \\", $force_rebuild [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nIt seems that vBulletin doesn\'t test on PHP 5.4 or 5.5 these days. Either that or they\'re happy to just suppress errors rather than actually fix them.  \\n\\nI upgraded my forum today to vBulletin 4.2.2 and noticed these errors on a search page:  \\n\\n```\\nWarning: Declaration of vBForum_Item_SocialGroupMessage::getLoadQuery() should be compatible with vB_Model::getLoadQuery($required_query = \\", $force_rebuild = false) in &#8230;./packages/vbforum/item/socialgroupmessage.php on line 261  \\n\\nWarning: Declaration of vBForum_Item_SocialGroupDiscussion::getLoadQuery() should be compatible with vB_Model::getLoadQuery($required_query = \\", $force_rebuild = false) in &#8230;./packages/vbforum/item/socialgroupdiscussion.php on line 337  \\n```\\n\\nLuckily a user on vbulletin.com support forum has a fix: [http://www.vbulletin.com/forum/forum/vbulletin-4/vbulletin-4-questions-problems-and-troubleshooting/4000233-warning-declaration-of-vbforum_item_socialgroupmessage?p=4000793#post4000793](http://www.vbulletin.com/forum/forum/vbulletin-4/vbulletin-4-questions-problems-and-troubleshooting/4000233-warning-declaration-of-vbforum_item_socialgroupmessage?p=4000793#post4000793)\\n\\nWhat annoys me is that vBulletin released this version a while ago, but are still distributing it with this code error."},{"id":"gpt-passback-tags-and-validation-errors","metadata":{"permalink":"/blog/gpt-passback-tags-and-validation-errors","source":"@site/blog/2013-12-16-gpt-passback-tags-and-validation-errors.mdx","title":"GPT Passback tags and validation errors","description":"Google released new passback tags for GPT (DFP) a while back. While these tags appear to work, they don\'t comply with W3C standards. It seems you can have either src or text between the tags, not both. [&hellip;]","date":"2013-12-16T00:00:00.000Z","formattedDate":"December 16, 2013","tags":[],"readingTime":0.715,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"gpt-passback-tags-and-validation-errors","title":"GPT Passback tags and validation errors"},"unlisted":false,"prevItem":{"title":"Another day, another vBulletin code error","permalink":"/blog/another-day-another-vbulletin-code-error"},"nextItem":{"title":"Xalan segfault","permalink":"/blog/xalan-segfault"}},"content":"<p>Google released new passback tags for GPT (DFP) a while back. While these tags appear to work, they don\'t comply with W3C standards. It seems you can have either src or text between the tags, not both. [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nGoogle released new passback tags for GPT (DFP) a while back. While these tags appear to work, they don\'t comply with W3C standards.\xa0E.g  \\n\\n```\\n<script src=\\"//www.googletagservices.com/tag/js/gpt.js\\">\\ngoogletag.pubads().definePassback(\'/7146/adunit\', [300, 250]).display();\\n<\/script>\\n```\\n\\nIt seems you can have either src or text between the tags, not both. It generates this error:  \\n\\n```\\nline 533 column 9 - Error: The text content of element script was not in the required format: Expected space, tab, newline, or slash but found g instead.  \\n```\\nI\'m unsure of a solution at the moment. I have raised this issue with our account manager, but I don\'t expect any fixes anytime soon."},{"id":"xalan-segfault","metadata":{"permalink":"/blog/xalan-segfault","source":"@site/blog/2013-09-30-xalan-segfault.mdx","title":"Xalan segfault","description":"When using xalan-c 1.10 and the supporting package xerces-c (3.0.1) from EPEL, Xalan would segfault when transforming xml with xslt. E.g. [root@box generate]# Xalan test.xml test.xsl Segmentation fault (core dumped) /var/log/messages didn\'t have any helpful information52 Xalan[25236]: segfault at 18 ip 00007f5b44758cb9 sp 00007fffa8ff33d0 error 4 in libxalan-c.so.110.0[7f5b444d3000+3e2000] There seems [&hellip;]","date":"2013-09-30T00:00:00.000Z","formattedDate":"September 30, 2013","tags":[],"readingTime":1.185,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"xalan-segfault","title":"Xalan segfault"},"unlisted":false,"prevItem":{"title":"GPT Passback tags and validation errors","permalink":"/blog/gpt-passback-tags-and-validation-errors"},"nextItem":{"title":"OpenSSL 1.0.1 for RHEL/CentOS 6.x","permalink":"/blog/openssl-1-0-1-for-rhelcentos-6-x"}},"content":"<p>When using xalan-c 1.10 and the supporting package xerces-c (3.0.1) from EPEL, Xalan would segfault when transforming xml with xslt. E.g. [root@box generate]# Xalan test.xml test.xsl Segmentation fault (core dumped) /var/log/messages didn\'t have any helpful information: Sep 30 17:52:01 box kernel: Xalan[25236]: segfault at 18 ip 00007f5b44758cb9 sp 00007fffa8ff33d0 error 4 in libxalan-c.so.110.0[7f5b444d3000+3e2000] There seems [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWhen using xalan-c 1.10 and the supporting package xerces-c (3.0.1) from EPEL, Xalan would segfault when transforming xml with xslt. E.g.  \\n\\n```\\n[root@box generate]# Xalan test.xml test.xsl\\n Segmentation fault (core dumped)\\n ```\\n\\n/var/log/messages didn\'t have any helpful information:  \\n\\n```\\nSep 30 17:52:01 box kernel: Xalan[25236]: segfault at 18 ip 00007f5b44758cb9 sp 00007fffa8ff33d0 error 4 in libxalan-c.so.110.0[7f5b444d3000+3e2000]\\n```\\n\\nThere seems to be a bug open for this at EPEL ([Bug\xa0807816](https://bugzilla.redhat.com/show_bug.cgi?id=807816) - Xalan-c segfaults on any input), but it has not been acknowledged or worked on.  \\n\\nI traced the problem to an incompatibility between xalan-c 1.10 and xerces-c 3.x. There is a patch as part of the EPEL xalan-c rpm which is meant to allow for this, but it seems broken as the source rpm didn\'t compile for me.  \\n\\nAn easy fix here is to upgrade both xalan-c and xerces-c to the latest version. I hacked together rpms for these based on the work already done in EPEL:  \\n\\n<a href=\\"http://www.noodles.net.nz/downloads/xalan-c-1.11.0-1.el6.src.rpm\\">xalan-c-1.11.0-1.el6.src.rpm</a>\\n<a href=\\"http://www.noodles.net.nz/downloads/xerces-c-3.1.1-1.el6.src.rpm\\">xerces-c-3.1.1-1.el6.src.rpm</a>  \\n\\nAfter initial testing it seems that this fixes the problem and XML can now be transformed in Xalan with XSLT"},{"id":"openssl-1-0-1-for-rhelcentos-6-x","metadata":{"permalink":"/blog/openssl-1-0-1-for-rhelcentos-6-x","source":"@site/blog/2013-09-26-openssl-1-0-1-for-rhelcentos-6-x.mdx","title":"OpenSSL 1.0.1 for RHEL/CentOS 6.x","description":"This is a great page on how to build OpenSSL 1.0.1 for RHEL/CentOS 6.x//www.ptudor.net/linux/openssl/ This has been ported from the work done on Fedora OpenSSL (https//pkgs.fedoraproject.org/cgit/openssl.git/). FIPS has been removed for ease of compiling. Have been running this on our dev environment for a few days and seems to be secure. OpenSSL [&hellip;]","date":"2013-09-26T00:00:00.000Z","formattedDate":"September 26, 2013","tags":[],"readingTime":0.62,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"openssl-1-0-1-for-rhelcentos-6-x","title":"OpenSSL 1.0.1 for RHEL/CentOS 6.x"},"unlisted":false,"prevItem":{"title":"Xalan segfault","permalink":"/blog/xalan-segfault"},"nextItem":{"title":"Edgecast Transact","permalink":"/blog/edgecast-transact"}},"content":"<p>This is a great page on how to build OpenSSL 1.0.1 for RHEL/CentOS 6.x: https://www.ptudor.net/linux/openssl/ This has been ported from the work done on Fedora OpenSSL (https://admin.fedoraproject.org/pkgdb/acls/name/openssl and http://pkgs.fedoraproject.org/cgit/openssl.git/). FIPS has been removed for ease of compiling. Have been running this on our dev environment for a few days and seems to be secure. OpenSSL [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nThis is a great page on how to build OpenSSL 1.0.1 for RHEL/CentOS 6.x:  \\n\\n<a href=\\"https://www.ptudor.net/linux/openssl/\\">https://www.ptudor.net/linux/openssl/</a>  \\n\\nThis has been ported from the work done on Fedora OpenSSL (<a href=\\"https://admin.fedoraproject.org/pkgdb/acls/name/openssl \\">https://admin.fedoraproject.org/pkgdb/acls/name/openssl</a> and <a href=\\"http://pkgs.fedoraproject.org/cgit/openssl.git/\\">http://pkgs.fedoraproject.org/cgit/openssl.git/</a>). FIPS has been removed for ease of compiling.  \\n\\nHave been running this on our dev environment for a few days and seems to be secure. OpenSSL 1.0.1 adds TLS 1.1/1.2 (changelog here: <a href=\\"http://www.openssl.org/news/changelog.html\\">http://www.openssl.org/news/changelog.html</a>)"},{"id":"edgecast-transact","metadata":{"permalink":"/blog/edgecast-transact","source":"@site/blog/2013-05-22-edgecast-transact.mdx","title":"Edgecast Transact","description":"So Edgecast has announced a new product today, a separate CDN just for e-commerce sites. As Telecom Ramblings puts it: The new network is based on their existing CDN technology, but built on an entirely separate network infrastructure tuned specifically for the site acceleration and transaction needs of online retail sites. In other words, it\u2019s [&hellip;]","date":"2013-05-22T00:00:00.000Z","formattedDate":"May 22, 2013","tags":[],"readingTime":1.165,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"edgecast-transact","title":"Edgecast Transact"},"unlisted":false,"prevItem":{"title":"OpenSSL 1.0.1 for RHEL/CentOS 6.x","permalink":"/blog/openssl-1-0-1-for-rhelcentos-6-x"},"nextItem":{"title":"Chorus Cabinet locations","permalink":"/blog/chorus-cabinet-locations"}},"content":"<p>So Edgecast has announced a new product today, a separate CDN just for e-commerce sites. As Telecom Ramblings puts it: The new network is based on their existing CDN technology, but built on an entirely separate network infrastructure tuned specifically for the site acceleration and transaction needs of online retail sites. In other words, it\u2019s [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nSo Edgecast has announced a new product today, a separate CDN just for e-commerce sites.  \\n\\nAs [Telecom Ramblings](https://www.telecomramblings.com/2013/05/edgecast-launches-ecommerce-acceleration-network/) puts it:  \\n\\n```\\nThe new network is based on their existing CDN technology, but built on an entirely separate network infrastructure tuned specifically for the site acceleration and transaction needs of online retail sites. In other words, it\u2019s aimed at enterprises tired of sharing a least-common-demoninator fast lane with everything from cute cat videos to gaming updates to whatever it is kids listen to these days.  \\n```\\n\\nAm I the only one who reads this as a lack of confidence in their core CDN product or are they trying to differentiate themselves from other CDNs? To me a CDN should be able to handle any traffic that you throw at it and if you are getting slow downs then it\'s time to find a new CDN.  \\n\\nI would rather my CDN put more time and money into their core product than branching off and building a completely separate network. What\'s next, a sports CDN? News CDN? Porn CDN?"},{"id":"chorus-cabinet-locations","metadata":{"permalink":"/blog/chorus-cabinet-locations","source":"@site/blog/2013-05-16-chorus-cabinet-locations.mdx","title":"Chorus Cabinet locations","description":"Here are the list of the existing Chorus (previously Telecom NZ) cabinets//www.chorus.co.nz/file/3194/existingdistributioncabinetlistmay2012.xlsx The coordinates of the cabinets are in NZ Map Grid format and can be converted here//apps.linz.govt.nz/coordinate-conversion/index.aspx?IS=NZMG&amp;OS=WGS84&amp;IO=NE&amp;IC=H&amp;IH=-&amp;OO=NE&amp;OC=H&amp;OH=-&amp;PN=N&amp;IF=T&amp;ID=+&amp;OF=H&amp;OD=+&amp;CI=Y&amp;doentry=Enter+coordinates&amp;DEBUG=&amp;ADVANCED=0 (with X being the Easting and Y being the Northing)","date":"2013-05-16T00:00:00.000Z","formattedDate":"May 16, 2013","tags":[],"readingTime":0.415,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"chorus-cabinet-locations","title":"Chorus Cabinet locations"},"unlisted":false,"prevItem":{"title":"Edgecast Transact","permalink":"/blog/edgecast-transact"},"nextItem":{"title":"Kraken Crawler","permalink":"/blog/kraken-crawler"}},"content":"<p>Here are the list of the existing Chorus (previously Telecom NZ) cabinets: http://www.chorus.co.nz/file/3194/existing_distribution_cabinet_list_may_2012.xlsx The coordinates of the cabinets are in NZ Map Grid format and can be converted here: http://apps.linz.govt.nz/coordinate-conversion/index.aspx?IS=NZMG&amp;OS=WGS84&amp;IO=NE&amp;IC=H&amp;IH=-&amp;OO=NE&amp;OC=H&amp;OH=-&amp;PN=N&amp;IF=T&amp;ID=+&amp;OF=H&amp;OD=+&amp;CI=Y&amp;do_entry=Enter+coordinates&amp;DEBUG=&amp;ADVANCED=0 (with X being the Easting and Y being the Northing)</p>\\n\\n\\n\x3c!--truncate--\x3e\\nHere are the list of the existing Chorus (previously Telecom NZ) cabinets: <a href=\\"http://www.chorus.co.nz/file/3194/existing_distribution_cabinet_list_may_2012.xlsx\\">http://www.chorus.co.nz/file/3194/existing_distribution_cabinet_list_may_2012.xlsx</a>  \\n\\nThe coordinates of the cabinets are in NZ Map Grid format and can be converted here: <a href=\\"http://apps.linz.govt.nz/coordinate-conversion/index.aspx?IS=NZMG&amp;OS=WGS84&amp;IO=NE&amp;IC=H&amp;IH=-&amp;OO=NE&amp;OC=H&amp;OH=-&amp;PN=N&amp;IF=T&amp;ID=+&amp;OF=H&amp;OD=+&amp;CI=Y&amp;do_entry=Enter+coordinates&amp;DEBUG=&amp;ADVANCED=0\\">http://apps.linz.govt.nz/coordinate-conversion/index.aspx?IS=NZMG&amp;OS=WGS84&amp;IO=NE&amp;IC=H&amp;IH=-&amp;OO=NE&amp;OC=H&amp;OH=-&amp;PN=N&amp;IF=T&amp;ID=+&amp;OF=H&amp;OD=+&amp;CI=Y&amp;do_entry=Enter+coordinates&amp;DEBUG=&amp;ADVANCED=0</a> (with X being the Easting and Y being the Northing)"},{"id":"kraken-crawler","metadata":{"permalink":"/blog/kraken-crawler","source":"@site/blog/2013-03-25-kraken-crawler.mdx","title":"Kraken Crawler","description":"Recently I have seen \\"kraken-crawler/0.2.0\\" hitting my site. This is a bot used by Kontera (advertising company) to \\"better understand and analyze your site\u2019s content\\" (according to their support staff). Apparently the crawler adheres to robots.txt so you can block it by adding kraken-crawler/* Disallow: / The URL to their crawler info is broken [&hellip;]","date":"2013-03-25T00:00:00.000Z","formattedDate":"March 25, 2013","tags":[],"readingTime":0.685,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"kraken-crawler","title":"Kraken Crawler"},"unlisted":false,"prevItem":{"title":"Chorus Cabinet locations","permalink":"/blog/chorus-cabinet-locations"},"nextItem":{"title":"s3fs/fuse on Centos/RHEL","permalink":"/blog/s3fsfuse-on-centosrhel"}},"content":"<p>Recently I have seen \\"kraken-crawler/0.2.0\\" hitting my site. This is a bot used by Kontera (advertising company) to \\"better understand and analyze your site\u2019s content\\" (according to their support staff). Apparently the crawler adheres to robots.txt so you can block it by adding: User-agent: kraken-crawler/* Disallow: / The URL to their crawler info is broken [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nRecently I have seen \\"kraken-crawler/0.2.0\\" hitting my site. This is a bot used by Kontera (advertising company) to \\"better understand and analyze your site\u2019s content\\" (according to their support staff).   \\n\\nApparently the crawler adheres to robots.txt so you can block it by adding:  \\n\\nUser-agent: kraken-crawler/*<br />\\nDisallow: /  \\n\\nThe URL to their crawler info is broken so it\'s hard to get an idea of what this is used for. If you are also seeing this bot, hopefully this helps you."},{"id":"s3fsfuse-on-centosrhel","metadata":{"permalink":"/blog/s3fsfuse-on-centosrhel","source":"@site/blog/2013-02-27-s3fsfuse-on-centosrhel.mdx","title":"s3fs/fuse on Centos/RHEL","description":"s3fs requires fuse 2.8.4, but on RHEL the latest version is 2.8.3, so fuse needs to be installed from source code. yum remove fuse fuse fuse-devel yum install gcc libstdc++-devel gcc-c++ curl curl curl-devel libxml2 libxml2* libxml2-devel openssl-devel mailcap wget \\"https/usr/lib64/pkgconfig/ ldconfig modprobe [&hellip;]","date":"2013-02-27T00:00:00.000Z","formattedDate":"February 27, 2013","tags":[],"readingTime":0.905,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"s3fsfuse-on-centosrhel","title":"s3fs/fuse on Centos/RHEL"},"unlisted":false,"prevItem":{"title":"Kraken Crawler","permalink":"/blog/kraken-crawler"},"nextItem":{"title":"IE10 for Windows 7 (a tale of SPOF)","permalink":"/blog/ie10-for-windows-7-a-tale-of-spof"}},"content":"<p>s3fs requires fuse 2.8.4, but on RHEL the latest version is 2.8.3, so fuse needs to be installed from source code. yum remove fuse fuse* fuse-devel yum install gcc libstdc++-devel gcc-c++ curl curl* curl-devel libxml2 libxml2* libxml2-devel openssl-devel mailcap wget \\"https://downloads.sourceforge.net/project/fuse/fuse-2.X/2.8.4/fuse-2.8.4.tar.gz?r=&amp;ts=1299709935&amp;use_mirror=cdnetworks-us-1\\" tar -xzf fuse-2.8.4.tar.gz cd fuse-2.8.4/ ./configure &#8211;prefix=/usr make make install export PKG_CONFIG_PATH=/usr/lib/pkgconfig:/usr/lib64/pkgconfig/ ldconfig modprobe [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\ns3fs requires fuse 2.8.4, but on RHEL the latest version is 2.8.3, so fuse needs to be installed from source code.  \\n\\n```\\nyum remove fuse fuse* fuse-devel\\nyum install gcc libstdc++-devel gcc-c++ curl curl* curl-devel libxml2 libxml2* libxml2-devel openssl-devel mailcap  \\n\\nwget \\"https://downloads.sourceforge.net/project/fuse/fuse-2.X/2.8.4/fuse-2.8.4.tar.gz?r=&amp;ts=1299709935&amp;use_mirror=cdnetworks-us-1\\"<br />\\ntar -xzf fuse-2.8.4.tar.gz<br />\\ncd fuse-2.8.4/<br />\\n./configure &#8211;prefix=/usr<br />\\nmake<br />\\nmake install<br />\\nexport PKG_CONFIG_PATH=/usr/lib/pkgconfig:/usr/lib64/pkgconfig/<br />\\nldconfig<br />\\nmodprobe fuse<br />\\npkg-config &#8211;modversion fuse<br />\\ncd ../<br />\\nwget http://s3fs.googlecode.com/files/s3fs-1.63.tar.gz<br />\\ntar -xzf s3fs-1.63.tar.gz<br />\\ncd s3fs-1.63<br />\\n./configure &#8211;prefix=/usr<br />\\nmake<br />\\nmake install  \\n</blockquote>\\nIf when reinstalling s3fs you get this error:  \\n\\n<blockquote>No package &#8216;fuse\' found  \\n</blockquote>\\nYou need to re-run this before compiling s3fs  \\n\\n<blockquote>export PKG_CONFIG_PATH=/usr/lib/pkgconfig:/usr/lib64/pkgconfig/<br />\\nldconfig<br />\\nmodprobe fuse<br />\\npkg-config &#8211;modversion fuse  \\n</blockquote>"},{"id":"ie10-for-windows-7-a-tale-of-spof","metadata":{"permalink":"/blog/ie10-for-windows-7-a-tale-of-spof","source":"@site/blog/2013-02-26-ie10-for-windows-7-a-tale-of-spof.mdx","title":"IE10 for Windows 7 (a tale of SPOF)","description":"Microsoft have finally released IE10 for Windows 7. It seems their download page (http://windows.microsoft.com/en-us/internet-explorer/downloads/ie-10/worldwide-languages) is getting pretty hammered. Looking at the requests on the page it seems that everything is held up with a request to ajax.microsoft.com. The page loads the template header, but no more. Surely in the day and age a company like [&hellip;]","date":"2013-02-26T00:00:00.000Z","formattedDate":"February 26, 2013","tags":[],"readingTime":0.825,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ie10-for-windows-7-a-tale-of-spof","title":"IE10 for Windows 7 (a tale of SPOF)"},"unlisted":false,"prevItem":{"title":"s3fs/fuse on Centos/RHEL","permalink":"/blog/s3fsfuse-on-centosrhel"},"nextItem":{"title":"AWS OpsWorks","permalink":"/blog/aws-opsworks"}},"content":"<p>Microsoft have finally released IE10 for Windows 7. It seems their download page (http://windows.microsoft.com/en-us/internet-explorer/downloads/ie-10/worldwide-languages) is getting pretty hammered. Looking at the requests on the page it seems that everything is held up with a request to ajax.microsoft.com. The page loads the template header, but no more. Surely in the day and age a company like [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nMicrosoft have finally released IE10 for Windows 7. It seems their download page (<a href=\\"http://windows.microsoft.com/en-us/internet-explorer/downloads/ie-10/worldwide-languages\\" target=\\"_blank\\">http://windows.microsoft.com/en-us/internet-explorer/downloads/ie-10/worldwide-languages</a>) is getting pretty hammered. Looking at the requests on the page it seems that everything is held up with a request to ajax.microsoft.com. The page loads the template header, but no more. Surely in the day and age a company like Microsoft would load their ajax async and prevent a single script from taking down the page.  \\n\\nUpdate: it seems this is a problem with the latest build of Firefox\'s Aurora. Twitter is experiencing a similar problem with one of their scripts, so there may be a problem with Firefox\'s script engine."},{"id":"aws-opsworks","metadata":{"permalink":"/blog/aws-opsworks","source":"@site/blog/2013-02-20-aws-opsworks.mdx","title":"AWS OpsWorks","description":"Amazon have released their new application management tool OpsWorks. This uses Chef to deploy and maintain instances on AWS. While it looks neat and I\'m sure it will work for startups it\'s not something I could trust. I still like to get my hands dirty with server deployment and I try to use bare metal [&hellip;]","date":"2013-02-20T00:00:00.000Z","formattedDate":"February 20, 2013","tags":[],"readingTime":1.415,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"aws-opsworks","title":"AWS OpsWorks"},"unlisted":false,"prevItem":{"title":"IE10 for Windows 7 (a tale of SPOF)","permalink":"/blog/ie10-for-windows-7-a-tale-of-spof"},"nextItem":{"title":"rpmrebuild ftw!","permalink":"/blog/rpmrebuild-ftw"}},"content":"<p>Amazon have released their new application management tool OpsWorks. This uses Chef to deploy and maintain instances on AWS. While it looks neat and I\'m sure it will work for startups it\'s not something I could trust. I still like to get my hands dirty with server deployment and I try to use bare metal [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nAmazon have released their new application management tool OpsWorks. This uses Chef to deploy and maintain instances on AWS. While it looks neat and I\'m sure it will work for startups it\'s not something I could trust. I still like to get my hands dirty with server deployment and I try to use bare metal rather than virtual instances where possible. Also, from what I\'m reading this tool is still very much a \\"beta\\" and is quite buggy.  \\n\\nThe tool itself is not revolutionary, there are many other systems out there that do a similar thing. What is interesting though is that Amazon is offering this, once again improving the tools available without the need to use a 3rd party. Will this kill off competition or prompt the current providers to lift their game?  \\n\\nOpsWorks has brought up an interesting question. Now that AWS is using Chef and they have thousands of developers/sites using them, will Chef become the defacto standard and will other configuration management systems die out? There is a rumour that Amazon might offer Puppet support along side Chef, but that\'s just a rumour for now.  \\n\\nPersonally I think Chef will increase in popularity due to OpsWorks, but I don\'t think Puppet et al will die away. Each system has their own merits and devs/ops will use whatever suits them and their environment."},{"id":"rpmrebuild-ftw","metadata":{"permalink":"/blog/rpmrebuild-ftw","source":"@site/blog/2013-02-18-rpmrebuild-ftw.mdx","title":"rpmrebuild ftw!","description":"There\'s always been a problem with Oracle provided MySQL rpms and older Centos/RHEL MySQL rpms. The former provides \\"MySQL\\" and the latter provides \\"mysql\\", so a lot of the packages in Centos/RHEL require \\"mysql\\" which creates some conflicts. A quick way to fix this is to use rpmrebuild -e -p and change the \\"requires\\" from [&hellip;]","date":"2013-02-18T00:00:00.000Z","formattedDate":"February 18, 2013","tags":[],"readingTime":0.675,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"rpmrebuild-ftw","title":"rpmrebuild ftw!"},"unlisted":false,"prevItem":{"title":"AWS OpsWorks","permalink":"/blog/aws-opsworks"},"nextItem":{"title":"New server with SSDs","permalink":"/blog/new-server-with-ssds"}},"content":"<p>There\'s always been a problem with Oracle provided MySQL rpms and older Centos/RHEL MySQL rpms. The former provides \\"MySQL\\" and the latter provides \\"mysql\\", so a lot of the packages in Centos/RHEL require \\"mysql\\" which creates some conflicts. A quick way to fix this is to use rpmrebuild -e -p and change the \\"requires\\" from [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nThere\'s always been a problem with Oracle provided MySQL rpms and older Centos/RHEL MySQL rpms. The former provides \\"MySQL\\" and the latter provides \\"mysql\\", so a lot of the packages in Centos/RHEL require \\"mysql\\" which creates some conflicts.  \\n\\nA quick way to fix this is to use `rpmrebuild -e -p <rpmfile>` and change the \\"requires\\" from \\"mysql\\" to \\"MySQL\\". Hopefully in the future Centos/RHEL will be standardized with the Oracle naming convention or Oracle packages be \\"backwardly\\" compatible."},{"id":"new-server-with-ssds","metadata":{"permalink":"/blog/new-server-with-ssds","source":"@site/blog/2013-02-14-new-server-with-ssds.mdx","title":"New server with SSDs","description":"We just provisioned a new server with Sandy Bridge and 4 SSDs in RAID 5 configuration. The server it was replacing was seriously under powered so this is a timely replacement. I ran hdparm on both servers to compare dag Timing cached reads: 6678 MB in 2.00 seconds = [&hellip;]","date":"2013-02-14T00:00:00.000Z","formattedDate":"February 14, 2013","tags":[],"readingTime":0.87,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"new-server-with-ssds","title":"New server with SSDs"},"unlisted":false,"prevItem":{"title":"rpmrebuild ftw!","permalink":"/blog/rpmrebuild-ftw"},"nextItem":{"title":"Vbulletin 4.2.x and PHP 5.4","permalink":"/blog/vbulletin-4-2-x-and-php-5-4"}},"content":"<p>We just provisioned a new server with Sandy Bridge and 4 SSDs in RAID 5 configuration. The server it was replacing was seriously under powered so this is a timely replacement. I ran hdparm on both servers to compare: Old Server: dag:/home# hdparm -Tt /dev/sda6 /dev/sda6: Timing cached reads: 6678 MB in 2.00 seconds = [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWe just provisioned a new server with Sandy Bridge and 4 SSDs in RAID 5 configuration. The server it was replacing was seriously under powered so this is a timely replacement. I ran hdparm on both servers to compare:  \\n\\nOld Server:\\n```\\ndag:/home# hdparm -Tt /dev/sda6\\n\\n/dev/sda6:\\n Timing cached reads:   6678 MB in  2.00 seconds = 3341.64 MB/sec\\n Timing buffered disk reads: 186 MB in  3.03 seconds =  61.38 MB/sec\\n```\\n\\nNew Server:\\n```\\nroot@dagnew:/home# hdparm -Tt /dev/sda6  \\n\\n/dev/sda6:\\n Timing cached reads:   25048 MB in  2.00 seconds = 12539.88 MB/sec\\n Timing buffered disk reads: 1956 MB in  3.00 seconds = 651.75 MB/sec\\n```\\n\\nI\'ll be rolling out more of these when other servers are up for replacement."},{"id":"vbulletin-4-2-x-and-php-5-4","metadata":{"permalink":"/blog/vbulletin-4-2-x-and-php-5-4","source":"@site/blog/2013-02-13-vbulletin-4-2-x-and-php-5-4.mdx","title":"Vbulletin 4.2.x and PHP 5.4","description":"It seems that the latest versions of vbulletin are very broken in PHP 5.4 even though they state that \\"vBulletin 4.x requires PHP 5.2.0 or greater and MySQL 4.1.0 or greater\\" Most of the problems are from ESTRICT which is part of EALL in PHP 5.4, but vBulletin and Internet Brands (who own vBulletin) seem [&hellip;]","date":"2013-02-13T00:00:00.000Z","formattedDate":"February 13, 2013","tags":[],"readingTime":1.24,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"vbulletin-4-2-x-and-php-5-4","title":"Vbulletin 4.2.x and PHP 5.4"},"unlisted":false,"prevItem":{"title":"New server with SSDs","permalink":"/blog/new-server-with-ssds"},"nextItem":{"title":"Innodb Recovery","permalink":"/blog/innodb-recovery"}},"content":"<p>It seems that the latest versions of vbulletin are very broken in PHP 5.4 even though they state that \\"vBulletin 4.x requires PHP 5.2.0 or greater and MySQL 4.1.0 or greater\\" Most of the problems are from E_STRICT which is part of E_ALL in PHP 5.4, but vBulletin and Internet Brands (who own vBulletin) seem [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nIt seems that the latest versions of vbulletin are very broken in PHP 5.4 even though they state that \\"vBulletin 4.x requires PHP 5.2.0 or greater and MySQL 4.1.0 or greater\\" \\n\\nMost of the problems are from E_STRICT which is part of E_ALL in PHP 5.4, but vBulletin and Internet Brands (who own vBulletin) seem very slow to fix these problems. They even denied that it was a problem with vBulletin when I originally reported some of the errors in June 2012 stating \\"Closing this issue because it appears to be unrelated to vBulletin code.\\"  \\n\\nThey have since reopened the issue and it has been rolled up in a PHP 5.4 check task, but seems quite slow being that PHP 5.4 was released nearly a year ago and PHP 5.5 is due out soon.  \\n\\nSo to get vBulletin working without errors on my sites I have to modify and fix all of these problems. I wish I could contribute back to vBulletin or to its users so that this effort is not duplicated, but there doesn\'t seem to be a way to do it (hosting files on here would violate copyright)."},{"id":"innodb-recovery","metadata":{"permalink":"/blog/innodb-recovery","source":"@site/blog/2013-01-15-innodb-recovery.mdx","title":"Innodb Recovery","description":"I recently had a database server fail during a large DELETE query, this caused some problems with innodb\'s ibdata1. The index of this data file was different to what MySQL expected. As this wasn\'t one of our main servers I hadn\'t tuned innodb and all the innodb data was in the single ibdata1 file. [&hellip;]","date":"2013-01-15T00:00:00.000Z","formattedDate":"January 15, 2013","tags":[],"readingTime":1.08,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"innodb-recovery","title":"Innodb Recovery"},"unlisted":false,"prevItem":{"title":"Vbulletin 4.2.x and PHP 5.4","permalink":"/blog/vbulletin-4-2-x-and-php-5-4"},"nextItem":{"title":"Copy changed files from CVS repository","permalink":"/blog/copy-changed-files-from-cvs-repository"}},"content":"<p>I recently had a database server fail during a large DELETE query, this caused some problems with innodb\'s ibdata1. The index of this data file was different to what MySQL expected. As this wasn\'t one of our main servers I hadn\'t tuned innodb and all the innodb data was in the single ibdata1 file. [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI recently had a database server fail during a large DELETE query, this caused some problems with innodb\'s ibdata1. The index of this data file was different to what MySQL expected. As this wasn\'t one of our main servers I hadn\'t tuned innodb and all the innodb data was in the single ibdata1 file. The only way for me to start MySQL was to add this to my.cnf:  \\n\\n```\\ninnodb_force_recovery = 4  \\n```\\n\\nThis forced MySQL to ignore all innodb errors and I used mysqldump to extract all the data from the innodb tables. Innodb tables were found using the following query:  \\n\\n```\\nSELECT table_schema, table_name\\nFROM INFORMATION_SCHEMA.TABLES\\nWHERE engine = \'innodb\';  \\n```\\n\\nI stopped MySQL server again, removed the innodb_force_recovery, deleted the ibdata1 file and tuned innodb. I also made sure I added this to my.cnf:  \\n\\n```\\ninnodb_file_per_table = 1\\ninnodb_log_files_in_group\xa0\xa0\xa0\xa0\xa0 = 2  \\n```\\n\\nAll tables were loaded from the mysqldump backup files and everything is all happy again."},{"id":"copy-changed-files-from-cvs-repository","metadata":{"permalink":"/blog/copy-changed-files-from-cvs-repository","source":"@site/blog/2012-03-14-copy-changed-files-from-cvs-repository.mdx","title":"Copy changed files from CVS repository","description":"While converting my cvs repositories to git I needed to copy all the changed files from my working directory into a temp directory and then into my git working dir. This was the solution that I came up with [&hellip;]","date":"2012-03-14T00:00:00.000Z","formattedDate":"March 14, 2012","tags":[],"readingTime":0.8,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"copy-changed-files-from-cvs-repository","title":"Copy changed files from CVS repository"},"unlisted":false,"prevItem":{"title":"Innodb Recovery","permalink":"/blog/innodb-recovery"},"nextItem":{"title":"apr-utils 1.4.1 rpmbuild error (seg fault)","permalink":"/blog/apr-utils-1-4-1-rpmbuild-error-seg-fault"}},"content":"<p>While converting my cvs repositories to git I needed to copy all the changed files from my working directory into a temp directory and then into my git working dir. This was the solution that I came up with [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWhile converting my cvs repositories to git I needed to copy all the changed files from my working directory into a temp directory and then into my git working dir. This was the solution that I came up with:  \\n\\n```\\ncvs -qn update -d 2>/dev/null | grep \'^[M|\\\\?|P] \' | awk \'{print $2}\' > files.list while read file; do cp -r -f --parents \\"$file\\" /tmp/dest/; done < files.list\\n```\\n\\n\\nBasically, cvs -qn update -d does a mock update (doesn\'t change files), piped to grep to only list the changed files, use awk to exclude the M, P, ? etc, output to a file.  \\n\\nLoop over the lines in the new file using the cp --parents to preserve directory structure"},{"id":"apr-utils-1-4-1-rpmbuild-error-seg-fault","metadata":{"permalink":"/blog/apr-utils-1-4-1-rpmbuild-error-seg-fault","source":"@site/blog/2012-02-22-apr-utils-1-4-1-rpmbuild-error-seg-fault.mdx","title":"apr-utils 1.4.1 rpmbuild error (seg fault)","description":"When trying to build an rpm for apr-utils on my CentOS 6.2 box I got a nasty error when the rpm was running test [&hellip;]","date":"2012-02-22T00:00:00.000Z","formattedDate":"February 22, 2012","tags":[],"readingTime":0.555,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"apr-utils-1-4-1-rpmbuild-error-seg-fault","title":"apr-utils 1.4.1 rpmbuild error (seg fault)"},"unlisted":false,"prevItem":{"title":"Copy changed files from CVS repository","permalink":"/blog/copy-changed-files-from-cvs-repository"},"nextItem":{"title":"RPM macros","permalink":"/blog/rpm-macros"}},"content":"<p>When trying to build an rpm for apr-utils on my CentOS 6.2 box I got a nasty error when the rpm was running test [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWhen trying to build an rpm for apr-utils on my CentOS 6.2 box I got a nasty error when the rpm was running test:  \\n\\n```\\ntestmemcache : |/bin/sh: line 2: 14322 Segmentation fault LD_LIBRARY_PATH=\\"`echo \\"../crypto/.libs:../dbm/.lib s:../dbd/.libs:../ldap/.libs:$LD_LIBRARY_PATH\\" | sed -e \'s/::*$//\'`\\" ./$prog\\nPrograms failed: testall\\nmake: *** [check] Error 139\\n+ exit 1\\nerror: Bad exit status from /var/tmp/rpm-tmp.OQddG8 (%check)  \\n```\\nThis relates to this bug: <a href=\\"https://issues.apache.org/bugzilla/show_bug.cgi?id=52705\\">https://issues.apache.org/bugzilla/show_bug.cgi?id=52705</a>  \\n\\nThanks to Peter Poeml for releasing a patch for this, which I\'ve put into an updated <a href=\\"http://www.noodles.net.nz/files/apr-util.spec\\">apr-util.spec</a>"},{"id":"rpm-macros","metadata":{"permalink":"/blog/rpm-macros","source":"@site/blog/2012-02-17-rpm-macros.mdx","title":"RPM macros","description":"I have been building a lot of custom RPMs lately and I found this great resource which lists all of the macros that can be used in the spec files and what they equate to. http://www.zarb.org/~jasonc/macros.php","date":"2012-02-17T00:00:00.000Z","formattedDate":"February 17, 2012","tags":[],"readingTime":0.37,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"rpm-macros","title":"RPM macros"},"unlisted":false,"prevItem":{"title":"apr-utils 1.4.1 rpmbuild error (seg fault)","permalink":"/blog/apr-utils-1-4-1-rpmbuild-error-seg-fault"},"nextItem":{"title":"CVS error","permalink":"/blog/cvs-error"}},"content":"<p>I have been building a lot of custom RPMs lately and I found this great resource which lists all of the macros that can be used in the spec files and what they equate to. http://www.zarb.org/~jasonc/macros.php</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI have been building a lot of custom RPMs lately and I found this great resource which lists all of the macros that can be used in the spec files and what they equate to.  \\n\\n<a href=\\"http://www.zarb.org/~jasonc/macros.php\\">http://www.zarb.org/~jasonc/macros.php</a>"},{"id":"cvs-error","metadata":{"permalink":"/blog/cvs-error","source":"@site/blog/2012-02-10-cvs-error.mdx","title":"CVS error","description":"I recently changed to using the unix command line for cvs and changed all my cvs roots to instead of (tortoise prefers ssh). When I made the change, anytime I updated cvs I got this error [&hellip;]","date":"2012-02-10T00:00:00.000Z","formattedDate":"February 10, 2012","tags":[],"readingTime":0.66,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"cvs-error","title":"CVS error"},"unlisted":false,"prevItem":{"title":"RPM macros","permalink":"/blog/rpm-macros"},"nextItem":{"title":"Network Solutions and DNSSEC","permalink":"/blog/network-solutions-and-dnssec"}},"content":"<p>I recently changed to using the unix command line for cvs and changed all my cvs roots to :ext: instead of :ssh: (tortoise prefers ssh). When I made the change, anytime I updated cvs I got this error [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI recently changed to using the unix command line for cvs and changed all my cvs roots to `:ext:` instead of `:ssh:` (tortoise prefers ssh).  \\n\\nWhen I made the change, anytime I updated cvs I got this error:  \\n\\n```\\n/CVSROOTccess /cvsroot\\nNo such file or directory\\n```\\n\\nThis makes no sense. Luckily, after searching around I found this is a problem with DOS line breaks screwing with unix cvs. Running the following fixes the problem:  \\n\\n```\\ndos2unix `find . -name Root`\\ndos2unix `find . -name Entries`\\ndos2unix `find . -name Repository`  \\n```"},{"id":"network-solutions-and-dnssec","metadata":{"permalink":"/blog/network-solutions-and-dnssec","source":"@site/blog/2012-01-18-network-solutions-and-dnssec.mdx","title":"Network Solutions and DNSSEC","description":"It\'s interesting that not only does netsol not list any information what-so-ever on their knowledge base about DNSSEC, their support staff have no idea what it is either. Come on network solutions, sort it out otherwise I may have to move my domains to a registrar that does support DNSSEC.","date":"2012-01-18T00:00:00.000Z","formattedDate":"January 18, 2012","tags":[],"readingTime":0.505,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"network-solutions-and-dnssec","title":"Network Solutions and DNSSEC"},"unlisted":false,"prevItem":{"title":"CVS error","permalink":"/blog/cvs-error"},"nextItem":{"title":"VLC Airplay on Airport Express","permalink":"/blog/vlc-airplay-on-airport-express"}},"content":"<p>It\'s interesting that not only does netsol not list any information what-so-ever on their knowledge base about DNSSEC, their support staff have no idea what it is either. Come on network solutions, sort it out otherwise I may have to move my domains to a registrar that does support DNSSEC.</p>\\n\\n\\n\x3c!--truncate--\x3e\\nIt\'s interesting that not only does netsol not list any information what-so-ever on their knowledge base about DNSSEC, their support staff have no idea what it is either.  \\n\\nCome on network solutions, sort it out otherwise I may have to move my domains to a registrar that does support DNSSEC."},{"id":"vlc-airplay-on-airport-express","metadata":{"permalink":"/blog/vlc-airplay-on-airport-express","source":"@site/blog/2011-08-15-vlc-airplay-on-airport-express.mdx","title":"VLC Airplay on Airport Express","description":"There is a way to stream music to your airport express from VLC (means you don\'t have to install iTunes).","date":"2011-08-15T00:00:00.000Z","formattedDate":"August 15, 2011","tags":[],"readingTime":0.315,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"vlc-airplay-on-airport-express","title":"VLC Airplay on Airport Express"},"unlisted":false,"prevItem":{"title":"Network Solutions and DNSSEC","permalink":"/blog/network-solutions-and-dnssec"},"nextItem":{"title":"df not reporting correct disk usage","permalink":"/blog/df-not-reporting-correct-disk-usage"}},"content":"<p>There is a way to stream music to your airport express from VLC (means you don\'t have to install iTunes).</p>\\n\\n\\n\x3c!--truncate--\x3e\\nThere is a way to stream music to your airport express from VLC (means you don\'t have to install iTunes).  \\n\\nUnder preferences > all > stream output enter this in the \\"Default stream output chain\\":  \\n\\n```\\n#transcode{acodec=alac,channels=2,samplerate=44100}:raop{host=&lt;ip address of aiport express&gt;,volume=175}  \\n```"},{"id":"df-not-reporting-correct-disk-usage","metadata":{"permalink":"/blog/df-not-reporting-correct-disk-usage","source":"@site/blog/2011-07-27-df-not-reporting-correct-disk-usage.mdx","title":"df not reporting correct disk usage","description":"I had a problem where df and du disagreed with the amount of disk usage. The cause was processes holding on to unlinked files. Running the following identified the processes: ls -ld /proc//fd/ 2&gt;&amp;1 | fgrep &#8216;(deleted)\' I killed the processes and df is now showing the correct information.","date":"2011-07-27T00:00:00.000Z","formattedDate":"July 27, 2011","tags":[],"readingTime":0.495,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"df-not-reporting-correct-disk-usage","title":"df not reporting correct disk usage"},"unlisted":false,"prevItem":{"title":"VLC Airplay on Airport Express","permalink":"/blog/vlc-airplay-on-airport-express"},"nextItem":{"title":"Dreamhost supports ipv6","permalink":"/blog/dreamhost-supports-ipv6"}},"content":"<p>I had a problem where df and du disagreed with the amount of disk usage. The cause was processes holding on to unlinked files. Running the following identified the processes: ls -ld /proc/*/fd/* 2&gt;&amp;1 | fgrep &#8216;(deleted)\' I killed the processes and df is now showing the correct information.</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI had a problem where df and du disagreed with the amount of disk usage. The cause was processes holding on to unlinked files. Running the following identified the processes:  \\n\\nls -ld /proc/*/fd/* 2&gt;&amp;1 | fgrep &#8216;(deleted)\'  \\n\\nI killed the processes and df is now showing the correct information."},{"id":"dreamhost-supports-ipv6","metadata":{"permalink":"/blog/dreamhost-supports-ipv6","source":"@site/blog/2011-04-29-dreamhost-supports-ipv6.mdx","title":"Dreamhost supports ipv6","description":"I just found out that dreamhost supports ipv6 (has done since March \'11). Just go to manage domains in the control panel and click \\"Add IP\\" next to each of your domains. Now if only our enterprise hosting at Verio/NTT supported ipv6!","date":"2011-04-29T00:00:00.000Z","formattedDate":"April 29, 2011","tags":[],"readingTime":0.425,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"dreamhost-supports-ipv6","title":"Dreamhost supports ipv6"},"unlisted":false,"prevItem":{"title":"df not reporting correct disk usage","permalink":"/blog/df-not-reporting-correct-disk-usage"},"nextItem":{"title":"Browser Wars (sunspider edition)","permalink":"/blog/browser-wars-sunspider-edition"}},"content":"<p>I just found out that dreamhost supports ipv6 (has done since March \'11). Just go to manage domains in the control panel and click \\"Add IP\\" next to each of your domains. Now if only our enterprise hosting at Verio/NTT supported ipv6!</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI just found out that dreamhost supports ipv6 (has done since March \'11). Just go to manage domains in the control panel and click \\"Add IP\\" next to each of your domains.  \\n\\nNow if only our enterprise hosting at Verio/NTT supported ipv6!"},{"id":"browser-wars-sunspider-edition","metadata":{"permalink":"/blog/browser-wars-sunspider-edition","source":"@site/blog/2011-04-14-browser-wars-sunspider-edition.mdx","title":"Browser Wars (sunspider edition)","description":"It seems that most browsers are focused on speeding up javascript execution now as most websites are heavy with ajax and complex js. With Firefox releasing their pre-beta \\"dev\\" version Aurora this morning I decided to run all the current browsers through sunspider to see if any progress has been made. Chrome-dev: 294ms Aurora (FF [&hellip;]","date":"2011-04-14T00:00:00.000Z","formattedDate":"April 14, 2011","tags":[],"readingTime":0.765,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"browser-wars-sunspider-edition","title":"Browser Wars (sunspider edition)"},"unlisted":false,"prevItem":{"title":"Dreamhost supports ipv6","permalink":"/blog/dreamhost-supports-ipv6"},"nextItem":{"title":"Remove site from Google","permalink":"/blog/remove-site-from-google"}},"content":"<p>It seems that most browsers are focused on speeding up javascript execution now as most websites are heavy with ajax and complex js. With Firefox releasing their pre-beta \\"dev\\" version Aurora this morning I decided to run all the current browsers through sunspider to see if any progress has been made. Chrome-dev: 294ms Aurora (FF [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nIt seems that most browsers are focused on speeding up javascript execution now as most websites are heavy with ajax and complex js.  \\n\\nWith Firefox releasing their pre-beta \\"dev\\" version Aurora this morning I decided to run all the current browsers through sunspider to see if any progress has been made.  \\n\\nChrome-dev: 294ms<br />\\nAurora (FF dev): 324ms<br />\\nFirefox 4: 348ms<br />\\nIE9: 251ms<br />\\nIE10 preview 1: 254ms  \\n\\nSo it would seem that Firefox has sped up their js engine, but has a little way to go to catch up to the other browsers."},{"id":"remove-site-from-google","metadata":{"permalink":"/blog/remove-site-from-google","source":"@site/blog/2011-02-22-remove-site-from-google.mdx","title":"Remove site from Google","description":"We found that Google had indexed a site that shouldn\'t be indexed so I setup a robots.txt file to deny all crawlers and locked down the site with http auth. I also put in a request to have the urls removed from the index and cache. When I did this Google returned ~2,400 results when [&hellip;]","date":"2011-02-22T00:00:00.000Z","formattedDate":"February 22, 2011","tags":[],"readingTime":0.875,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"remove-site-from-google","title":"Remove site from Google"},"unlisted":false,"prevItem":{"title":"Browser Wars (sunspider edition)","permalink":"/blog/browser-wars-sunspider-edition"},"nextItem":{"title":"Change CVS path","permalink":"/blog/change-cvs-path"}},"content":"<p>We found that Google had indexed a site that shouldn\'t be indexed so I setup a robots.txt file to deny all crawlers and locked down the site with http auth. I also put in a request to have the urls removed from the index and cache. When I did this Google returned ~2,400 results when [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWe found that Google had indexed a site that shouldn\'t be indexed so I setup a robots.txt file to deny all crawlers and locked down the site with http auth. I also put in a request to have the urls removed from the index and cache.  \\n\\nWhen I did this Google returned ~2,400 results when doing a \\"site:www.site.com\\". A few days later it was returning ~54,000. Today it is returning ~133,000.  \\n\\nI\'m not sure how Google managed to mix up \\"remove my site\\" with \\"index it more\\". Maybe this is just part of the removal process?  \\n\\n<strong>Update:</strong> Google is now up to 217,000 results for this site. Maybe removing your site from the index is good for SEO?"},{"id":"change-cvs-path","metadata":{"permalink":"/blog/change-cvs-path","source":"@site/blog/2011-02-16-change-cvs-path.mdx","title":"Change CVS path","description":"Found a small command to run through code directories and change the CVS path, handy if you\'re changing CVS username or path. find . -name &#8216;Root\' -exec perl -pi -e &#8216;s/OLDURL/NEWURL/\' {} \\\\;","date":"2011-02-16T00:00:00.000Z","formattedDate":"February 16, 2011","tags":[],"readingTime":0.335,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"change-cvs-path","title":"Change CVS path"},"unlisted":false,"prevItem":{"title":"Remove site from Google","permalink":"/blog/remove-site-from-google"},"nextItem":{"title":"Deny access to website, but allow robots.txt","permalink":"/blog/deny-access-to-website-but-allow-robots-txt"}},"content":"<p>Found a small command to run through code directories and change the CVS path, handy if you\'re changing CVS username or path. find . -name &#8216;Root\' -exec perl -pi -e &#8216;s/OLD_URL/NEW_URL/\' {} \\\\;</p>\\n\\n\\n\x3c!--truncate--\x3e\\nFound a small command to run through code directories and change the CVS path, handy if you\'re changing CVS username or path.  \\n\\nfind . -name &#8216;Root\' -exec perl -pi -e &#8216;s/OLD_URL/NEW_URL/\' {} \\\\;"},{"id":"deny-access-to-website-but-allow-robots-txt","metadata":{"permalink":"/blog/deny-access-to-website-but-allow-robots-txt","source":"@site/blog/2011-02-15-deny-access-to-website-but-allow-robots-txt.mdx","title":"Deny access to website, but allow robots.txt","description":"I had a problem where Googlebot was indexing a development site, so we locked it down using apache basic http auth. Now Googlebot was being served with a 401 when accessing the site, but because it had no stored robots.txt it was persistently trying to crawl the site. Using the following allows anyone to access [&hellip;]","date":"2011-02-15T00:00:00.000Z","formattedDate":"February 15, 2011","tags":[],"readingTime":0.85,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"deny-access-to-website-but-allow-robots-txt","title":"Deny access to website, but allow robots.txt"},"unlisted":false,"prevItem":{"title":"Change CVS path","permalink":"/blog/change-cvs-path"},"nextItem":{"title":"unzip 6 for RHEL 5.6","permalink":"/blog/unzip-6-for-rhel-5-6"}},"content":"<p>I had a problem where Googlebot was indexing a development site, so we locked it down using apache basic http auth. Now Googlebot was being served with a 401 when accessing the site, but because it had no stored robots.txt it was persistently trying to crawl the site. Using the following allows anyone to access [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI had a problem where Googlebot was indexing a development site, so we locked it down using apache basic http auth. Now Googlebot was being served with a 401 when accessing the site, but because it had no stored robots.txt it was persistently trying to crawl the site.  \\n\\nUsing the following allows anyone to access robots.txt but denies access to the rest of the site:<br />\\n&lt;Directory \\"/home/username/www\\"&gt;<br />\\nAuthUserFile /home/username/.htpasswd<br />\\nAuthName \\"Client Access\\"<br />\\nAuthType Basic<br />\\nrequire valid-use  \\n\\n&lt;Files \\"robots.txt\\"&gt;<br />\\nAuthType Basic<br />\\nsatisfy any<br />\\n&lt;/Files&gt;<br />\\n&lt;/Directory&gt;  \\n\\nEventually Googlebot will get the hint and stop indexing the site and we can remove existing content using webmaster tools."},{"id":"unzip-6-for-rhel-5-6","metadata":{"permalink":"/blog/unzip-6-for-rhel-5-6","source":"@site/blog/2011-02-15-unzip-6-for-rhel-5-6.mdx","title":"unzip 6 for RHEL 5.6","description":"Redhat likes to run old packages with backported patches, but sometimes it\'s nice to have the latest version that includes new features. Unfortunately the unzip from RHEL yum can\'t unzip files &gt; 4GB. I found the src rpm and compiled binaries that work with RHEL 5.6 (may work with older). unzip-6.0-1.i386.rpm unzip-6.0-1.x86_64.rpm","date":"2011-02-15T00:00:00.000Z","formattedDate":"February 15, 2011","tags":[],"readingTime":0.54,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"unzip-6-for-rhel-5-6","title":"unzip 6 for RHEL 5.6"},"unlisted":false,"prevItem":{"title":"Deny access to website, but allow robots.txt","permalink":"/blog/deny-access-to-website-but-allow-robots-txt"},"nextItem":{"title":"Overheating Macbook Pro","permalink":"/blog/overheating-macbook-pro"}},"content":"<p>Redhat likes to run old packages with backported patches, but sometimes it\'s nice to have the latest version that includes new features. Unfortunately the unzip from RHEL yum can\'t unzip files &gt; 4GB. I found the src rpm and compiled binaries that work with RHEL 5.6 (may work with older). unzip-6.0-1.i386.rpm unzip-6.0-1.x86_64.rpm</p>\\n\\n\\n\x3c!--truncate--\x3e\\nRedhat likes to run old packages with backported patches, but sometimes it\'s nice to have the latest version that includes new features. Unfortunately the unzip from RHEL yum can\'t unzip files &gt; 4GB. I found the src rpm and compiled binaries that work with RHEL 5.6 (may work with older).  \\n\\n<a href=\\"http://www.noodles.net.nz/downloads/unzip-6.0-1.i386.rpm\\">unzip-6.0-1.i386.rpm</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/unzip-6.0-1.x86_64.rpm\\">unzip-6.0-1.x86_64.rpm</a>"},{"id":"overheating-macbook-pro","metadata":{"permalink":"/blog/overheating-macbook-pro","source":"@site/blog/2011-02-14-overheating-macbook-pro.mdx","title":"Overheating Macbook Pro","description":"I\'ve been having problems recently with it overheating. It\'s not much fun when trying to code on the couch with a boiling hot mac on your lap. I found a program called smcfancontrol which seems to take care of that. It\'s now under 50oC and coding it now more fun!","date":"2011-02-14T00:00:00.000Z","formattedDate":"February 14, 2011","tags":[],"readingTime":0.51,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"overheating-macbook-pro","title":"Overheating Macbook Pro"},"unlisted":false,"prevItem":{"title":"unzip 6 for RHEL 5.6","permalink":"/blog/unzip-6-for-rhel-5-6"},"nextItem":{"title":"Online Banking","permalink":"/blog/online-banking"}},"content":"<p>I\'ve been having problems recently with it overheating. It\'s not much fun when trying to code on the couch with a boiling hot mac on your lap. I found a program called smcfancontrol which seems to take care of that. It\'s now under 50oC and coding it now more fun!</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI\'ve been having problems recently with it overheating. It\'s not much fun when trying to code on the couch with a boiling hot mac on your lap.  \\n\\nI found a program called <a href=\\"http://www.macupdate.com/app/mac/23049/smcfancontrol\\">smcfancontrol</a> which seems to take care of that. It\'s now under 50oC and coding it now more fun!"},{"id":"online-banking","metadata":{"permalink":"/blog/online-banking","source":"@site/blog/2011-01-05-online-banking.mdx","title":"Online Banking","description":"Why is it that in this day and age it takes a day to transfer money from one bank to another? And if it\'s a weekend or public holiday it takes even longer. Do computers not work weekends now? Can someone please invent a protocol or process to allow bank transfers to be instant? Surely [&hellip;]","date":"2011-01-05T00:00:00.000Z","formattedDate":"January 5, 2011","tags":[],"readingTime":0.77,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"online-banking","title":"Online Banking"},"unlisted":false,"prevItem":{"title":"Overheating Macbook Pro","permalink":"/blog/overheating-macbook-pro"},"nextItem":{"title":"PANWA2204A WLAN 11g Router","permalink":"/blog/panwa2204a-wlan-11g-router"}},"content":"<p>Why is it that in this day and age it takes a day to transfer money from one bank to another? And if it\'s a weekend or public holiday it takes even longer. Do computers not work weekends now? Can someone please invent a protocol or process to allow bank transfers to be instant? Surely [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWhy is it that in this day and age it takes a day to transfer money from one bank to another? And if it\'s a weekend or public holiday it takes even longer. Do computers not work weekends now?  \\n\\nCan someone please invent a protocol or process to allow bank transfers to be instant? Surely it can\'t be that hard, it\'s just numbers/data being shifted around. I can transfer entire DVDs of information from one side of the world to the other in a short period of time, why can\'t a bank process things in real time?"},{"id":"panwa2204a-wlan-11g-router","metadata":{"permalink":"/blog/panwa2204a-wlan-11g-router","source":"@site/blog/2010-11-17-panwa2204a-wlan-11g-router.mdx","title":"PANWA2204A WLAN 11g Router","description":"I came a cross an unbranded router with the FCC ID PANWA2204A stamped on the bottom (with MAC addresses and serial number). If anyone else is trying to identify one of these it\'s a \\"RadioLabs High Power O2Link Broadband Turbo G Router\\".","date":"2010-11-17T00:00:00.000Z","formattedDate":"November 17, 2010","tags":[],"readingTime":0.43,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"panwa2204a-wlan-11g-router","title":"PANWA2204A WLAN 11g Router"},"unlisted":false,"prevItem":{"title":"Online Banking","permalink":"/blog/online-banking"},"nextItem":{"title":"Auckland Frost Dates","permalink":"/blog/auckland-frost-dates"}},"content":"<p>I came a cross an unbranded router with the FCC ID PANWA2204A stamped on the bottom (with MAC addresses and serial number). If anyone else is trying to identify one of these it\'s a \\"RadioLabs High Power O2Link Broadband Turbo G Router\\".</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI came a cross an unbranded router with the FCC ID PANWA2204A stamped on the bottom (with MAC addresses and serial number). If anyone else is trying to identify one of these it\'s a \\"<a href=\\"http://www.radiolabs.com/products/wireless/networking/radiolabs-wireless-router.php\\">RadioLabs High Power O2Link Broadband Turbo G Router</a>\\"."},{"id":"auckland-frost-dates","metadata":{"permalink":"/blog/auckland-frost-dates","source":"@site/blog/2010-07-05-auckland-frost-dates.mdx","title":"Auckland Frost Dates","description":"I had been searching for approximate frost dates for Auckland to help schedule veggie plantings. Luckily the people at NIWA are nice and respond very quickly to emails. I was told that ground frosts are days where grass minimum temperature are less than -1.0 degree C. Screen frosts are when the air temperature dips below [&hellip;]","date":"2010-07-05T00:00:00.000Z","formattedDate":"July 5, 2010","tags":[],"readingTime":1.255,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"auckland-frost-dates","title":"Auckland Frost Dates"},"unlisted":false,"prevItem":{"title":"PANWA2204A WLAN 11g Router","permalink":"/blog/panwa2204a-wlan-11g-router"},"nextItem":{"title":"Creating Access ODBC datasource under Windows 7 (64bit)","permalink":"/blog/creating-access-odbc-datasource-under-windows-7-64bit"}},"content":"<p>I had been searching for approximate frost dates for Auckland to help schedule veggie plantings. Luckily the people at NIWA are nice and respond very quickly to emails. I was told that ground frosts are days where grass minimum temperature are less than -1.0 degree C. Screen frosts are when the air temperature dips below [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI had been searching for approximate frost dates for Auckland to help schedule veggie plantings. Luckily the people at <a href=\\"http://www.niwa.co.nz\\">NIWA</a> are nice and respond very quickly to emails. I was told that ground frosts are days where grass minimum temperature are less than -1.0 degree C. Screen frosts are when the air temperature dips below 0.0 degrees C.  \\n\\nUsing NIWA\'s free data explorer tool \\"Cliflo\\" and the closest weather station to me I came up with the following data:  \\n\\n2009<br />\\nFirst Frost: 2009-04-10<br />\\nLast Frost: 2009-09-09  \\n\\n2008<br />\\nFirst Frost: 2008-04-21<br />\\nLast Frost: 2008-09-07  \\n\\n2007<br />\\nFirst Frost: 2007-05-29<br />\\nLast Frost: 2007-10-04  \\n\\n2006<br />\\nFirst Frost: 2006-05-31<br />\\nLast Frost: 2006-09-19  \\n\\n2005<br />\\nFirst Frost: 2005-04-22<br />\\nLast Frost: 2005-09-03  \\n\\n2004<br />\\nFirst Frost: 2004-05-21<br />\\nLast Frost: 2004-09-30  \\n\\n2003<br />\\nFirst Frost: 2003-04-26<br />\\nLast Frost: 2003-10-27  \\n\\n2002<br />\\nFirst Frost: 2002-07-14<br />\\nLast Frost: 2002-10-24  \\n\\n2001<br />\\nFirst Frost: 2001-05-29<br />\\nLast Frost: 2001-09-29  \\n\\n2000<br />\\nFirst Frost: 2000-06-14<br />\\nLast Frost: 2000-09-29  \\n\\n<h2>This averages out to:</h2>\\nFirst Frost: May 19<br />\\nLast Frost: September 27  \\n\\nHopefully this helps the local gardeners. Join me on <a href=\\"http://myfolia.com\\">Folia</a>"},{"id":"creating-access-odbc-datasource-under-windows-7-64bit","metadata":{"permalink":"/blog/creating-access-odbc-datasource-under-windows-7-64bit","source":"@site/blog/2010-06-01-creating-access-odbc-datasource-under-windows-7-64bit.mdx","title":"Creating Access ODBC datasource under Windows 7 (64bit)","description":"I had to create an ODBC datasource for an access database on my Windows 7 machine and for some reason the *.mdb driver doesn\'t show up in the control panel ODBC administrator. After much swearing and google-ing I found the solution. In C:\\\\windows\\\\sysWOW64 there is an executable called odbcad32.exe. You need to run this exe [&hellip;]","date":"2010-06-01T00:00:00.000Z","formattedDate":"June 1, 2010","tags":[],"readingTime":0.67,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"creating-access-odbc-datasource-under-windows-7-64bit","title":"Creating Access ODBC datasource under Windows 7 (64bit)"},"unlisted":false,"prevItem":{"title":"Auckland Frost Dates","permalink":"/blog/auckland-frost-dates"},"nextItem":{"title":"Google DNS and OpenDNS vs CDNs","permalink":"/blog/google-dns-and-opendns-vs-cdns"}},"content":"<p>I had to create an ODBC datasource for an access database on my Windows 7 machine and for some reason the *.mdb driver doesn\'t show up in the control panel ODBC administrator. After much swearing and google-ing I found the solution. In C:\\\\windows\\\\sysWOW64 there is an executable called odbcad32.exe. You need to run this exe [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI had to create an ODBC datasource for an access database on my Windows 7 machine and for some reason the *.mdb driver doesn\'t show up in the control panel ODBC administrator.  \\n\\nAfter much swearing and google-ing I found the solution.  \\n\\nIn C:\\\\windows\\\\sysWOW64 there is an executable called odbcad32.exe. You need to run this exe as administrator to gain access to all the ODBC drivers that come with Microsoft Office, etc.  \\n\\nThanks Microsoft for another fine product!"},{"id":"google-dns-and-opendns-vs-cdns","metadata":{"permalink":"/blog/google-dns-and-opendns-vs-cdns","source":"@site/blog/2009-12-04-google-dns-and-opendns-vs-cdns.mdx","title":"Google DNS and OpenDNS vs CDNs","description":"With more and more people moving towards using a global DNS system (like Google DNS and OpenDNS) the speed advantages of a CDN may be cancelled out. Most of the major CDNs use geotargeting based on where the DNS is being resolved from. For example Facebook\'s CDN solution (using Akamai) resolves static.ak.fbcdn.net to 210.55.501.200, a [&hellip;]","date":"2009-12-04T00:00:00.000Z","formattedDate":"December 4, 2009","tags":[],"readingTime":0.905,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"google-dns-and-opendns-vs-cdns","title":"Google DNS and OpenDNS vs CDNs"},"unlisted":false,"prevItem":{"title":"Creating Access ODBC datasource under Windows 7 (64bit)","permalink":"/blog/creating-access-odbc-datasource-under-windows-7-64bit"},"nextItem":{"title":"Windows XP 32bit drivers for Dell Studio XPS 435MT","permalink":"/blog/windows-xp-32bit-drivers-for-dell-studio-xps-435mt"}},"content":"<p>With more and more people moving towards using a global DNS system (like Google DNS and OpenDNS) the speed advantages of a CDN may be cancelled out. Most of the major CDNs use geotargeting based on where the DNS is being resolved from. For example Facebook\'s CDN solution (using Akamai) resolves static.ak.fbcdn.net to 210.55.501.200, a [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWith more and more people moving towards using a global DNS system (like Google DNS and OpenDNS) the speed advantages of a CDN may be cancelled out.  \\n\\nMost of the major CDNs use geotargeting based on where the DNS is being resolved from. For example Facebook\'s CDN solution (using Akamai) resolves static.ak.fbcdn.net to 210.55.501.200, a 5ms response time. Using Google\'s public DNS server (8.8.8.8) the same domain resolves to 72.246.43.42, 72.246.43.32, a 200ms response time.  \\n\\nSo while using Google\'s DNS/OpenDNS may save a few ms while resolving a domain, it may slow down a site by putting the CDN pops further away from end users. Until CDN\'s can work with these public DNS providers the internet may become slower for those using these services."},{"id":"windows-xp-32bit-drivers-for-dell-studio-xps-435mt","metadata":{"permalink":"/blog/windows-xp-32bit-drivers-for-dell-studio-xps-435mt","source":"@site/blog/2009-06-10-windows-xp-32bit-drivers-for-dell-studio-xps-435mt.mdx","title":"Windows XP 32bit drivers for Dell Studio XPS 435MT","description":"We just recieved a couple of new computers today, shiny new Dell Studio XPS 435MT boxes. As everyone here still runs XP 32bit I decided to reinstall the new computers with the same OS, so we didn\'t run into any compatability problems. The problem is Dell doesn\'t support any other operating systems on these computers [&hellip;]","date":"2009-06-10T00:00:00.000Z","formattedDate":"June 10, 2009","tags":[],"readingTime":0.845,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"windows-xp-32bit-drivers-for-dell-studio-xps-435mt","title":"Windows XP 32bit drivers for Dell Studio XPS 435MT"},"unlisted":false,"prevItem":{"title":"Google DNS and OpenDNS vs CDNs","permalink":"/blog/google-dns-and-opendns-vs-cdns"},"nextItem":{"title":"Webby Awards","permalink":"/blog/webby-awards"}},"content":"<p>We just recieved a couple of new computers today, shiny new Dell Studio XPS 435MT boxes. As everyone here still runs XP 32bit I decided to reinstall the new computers with the same OS, so we didn\'t run into any compatability problems. The problem is Dell doesn\'t support any other operating systems on these computers [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nWe just recieved a couple of new computers today, shiny new Dell Studio XPS 435MT boxes. As everyone here still runs XP 32bit I decided to reinstall the new computers with the same OS, so we didn\'t run into any compatability problems. The problem is Dell doesn\'t support any other operating systems on these computers other than Vista 64bit!  \\n\\nAfter searching around the internet for a couple of hours I found the drivers needed, they are as follows:  \\n\\nNetwork (Intel 82567):\\n<a href=\\"http://downloadcenter.intel.com/Detail_Desc.aspx?agr=Y&amp;ProductID=3003&amp;DwnldID=4275&amp;strOSs=44&amp;OSFullName=Windows*%20XP%20Professional&amp;lang=eng\\" target=\\"_blank\\">http://downloadcenter.intel.com/Detail_Desc.aspx?agr=Y&amp;ProductID=3003&amp;DwnldID=4275&amp;strOSs=44&amp;OSFullName=Windows*%20XP%20Professional&amp;lang=eng</a>  \\n\\nChipset (Intel X58):\\n<a href=\\"http://downloadcenter.intel.com/filter_results.aspx?strTypes=all&amp;ProductID=816&amp;OSFullName=Windows*+XP+Professional&amp;lang=eng&amp;strOSs=44&amp;submit=Go!\\" target=\\"_blank\\">http://downloadcenter.intel.com/filter_results.aspx?strTypes=all&amp;ProductID=816&amp;OSFullName=Windows*+XP+Professional&amp;lang=eng&amp;strOSs=44&amp;submit=Go!</a>  \\n\\nGraphics (ATI Radeon 4850):\\n<a href=\\"http://game.amd.com/us-en/drivers_catalyst.aspx?p=xp/radeonx-xp\\">http://game.amd.com/us-en/drivers_catalyst.aspx?p=xp/radeonx-xp</a>  \\n\\nAudio (Realtek HD Audio):\\n<a href=\\"http://www.realtek.com.tw/downloads/downloadsView.aspx?Langid=1&amp;PFid=24&amp;Level=4&amp;Conn=3&amp;DownTypeID=3\\" target=\\"_blank\\">http://www.realtek.com.tw/downloads/downloadsView.aspx?Langid=1&amp;PFid=24&amp;Level=4&amp;Conn=3&amp;DownTypeID=3</a>  \\n\\nI hope this helps anyone with a similar problem"},{"id":"webby-awards","metadata":{"permalink":"/blog/webby-awards","source":"@site/blog/2009-04-30-webby-awards.mdx","title":"Webby Awards","description":"The Webby Awards are upon us again for the 13th year. It\'s ironic though that awards that celebrate the best on the internet have one of the worst designed sites. The Webby People\'s Voice site (http://pv.webbyawards.com) is hard to register on and vote. This has discouraged a lot of people from voting for their favourite [&hellip;]","date":"2009-04-30T00:00:00.000Z","formattedDate":"April 30, 2009","tags":[],"readingTime":0.575,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"webby-awards","title":"Webby Awards"},"unlisted":false,"prevItem":{"title":"Windows XP 32bit drivers for Dell Studio XPS 435MT","permalink":"/blog/windows-xp-32bit-drivers-for-dell-studio-xps-435mt"},"nextItem":{"title":"Recursive file sizes (filtered by file type)","permalink":"/blog/recursive-file-sizes-filtered-by-file-type"}},"content":"<p>The Webby Awards are upon us again for the 13th year. It\'s ironic though that awards that celebrate the best on the internet have one of the worst designed sites. The Webby People\'s Voice site (http://pv.webbyawards.com) is hard to register on and vote. This has discouraged a lot of people from voting for their favourite [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nThe Webby Awards are upon us again for the 13th year. It\'s ironic though that awards that celebrate the best on the internet have one of the worst designed sites. The Webby People\'s Voice site (<a href=\\"http://pv.webbyawards.com\\" target=\\"_blank\\">http://pv.webbyawards.com</a>) is hard to register on and vote. This has discouraged a lot of people from voting for their favourite websites."},{"id":"recursive-file-sizes-filtered-by-file-type","metadata":{"permalink":"/blog/recursive-file-sizes-filtered-by-file-type","source":"@site/blog/2009-04-09-recursive-file-sizes-filtered-by-file-type.mdx","title":"Recursive file sizes (filtered by file type)","description":"I had to find out the size of all our images on a site today. This was the easiest and fastest way to do it","date":"2009-04-09T00:00:00.000Z","formattedDate":"April 9, 2009","tags":[],"readingTime":0.305,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"recursive-file-sizes-filtered-by-file-type","title":"Recursive file sizes (filtered by file type)"},"unlisted":false,"prevItem":{"title":"Webby Awards","permalink":"/blog/webby-awards"},"nextItem":{"title":"Disable SSLv2 in Webmin","permalink":"/blog/disable-sslv2-in-webmin"}},"content":"<p>I had to find out the size of all our images on a site today. This was the easiest and fastest way to do it</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI had to find out the size of all our images on a site today. This was the easiest and fastest way to do it  \\n\\n```\\nfind /home/username/public_html -name \'*.png\' -printf \\"%s\\\\n\\"|awk \'{sum+=$0}END{print sum}\'\\n```"},{"id":"disable-sslv2-in-webmin","metadata":{"permalink":"/blog/disable-sslv2-in-webmin","source":"@site/blog/2008-09-22-disable-sslv2-in-webmin.mdx","title":"Disable SSLv2 in Webmin","description":"I\'ve been battling with Webmin trying to get SSLv2 turned off so I can comply with Hackersafe/McAfee Secure. I managed to do it this morning, this is how I did it-SSLv2:-aNULL into the Allowed SSL Ciphers field (new as [&hellip;]","date":"2008-09-22T00:00:00.000Z","formattedDate":"September 22, 2008","tags":[],"readingTime":0.795,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"disable-sslv2-in-webmin","title":"Disable SSLv2 in Webmin"},"unlisted":false,"prevItem":{"title":"Recursive file sizes (filtered by file type)","permalink":"/blog/recursive-file-sizes-filtered-by-file-type"},"nextItem":{"title":"SiXXs down?","permalink":"/blog/sixxs-down"}},"content":"<p>I\'ve been battling with Webmin trying to get SSLv2 turned off so I can comply with Hackersafe/McAfee Secure. I managed to do it this morning, this is how I did it: &#8211; Upgrade to version 1.430 &#8211; Webmin -&gt; Webmin Configuration -&gt; SSL Encryption &#8211; Enter HIGH:-SSLv2:-aNULL into the Allowed SSL Ciphers field (new as [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI\'ve been battling with Webmin trying to get SSLv2 turned off so I can comply with Hackersafe/McAfee Secure.  \\n\\nI managed to do it this morning, this is how I did it:  \\n\\n&#8211; Upgrade to version 1.430<br />\\n&#8211; Webmin -&gt; Webmin Configuration -&gt; SSL Encryption<br />\\n&#8211; Enter HIGH:-SSLv2:-aNULL into the Allowed SSL Ciphers field (new as of 1.430)<br />\\n&#8211; Restart Webmin  \\n\\nYou can check that SSLv2 is disabled by running this from the shell/command line:<br />\\nopenssl s_client -connect localhost:10000 -ssl2  \\n\\nIf you get lines like these, SSLv2 is disabled:<br />\\n419:error:1407F0E5:SSL routines:SSL2_WRITE:ssl handshake failure:s2_pkt.c:428:<br />\\n420:error:1406D0B8:SSL routines:GET_SERVER_HELLO:no cipher list:s2_clnt.c:450:"},{"id":"sixxs-down","metadata":{"permalink":"/blog/sixxs-down","source":"@site/blog/2008-09-08-sixxs-down.mdx","title":"SiXXs down?","description":"For some reason Sixxs.net seems to be down. I\'m unable to connect my IPV6 tunnels to their PoP\'s or connect to their website. Does anyone have any idea why they\'re offline? Update: Seems to be back up now. It\'s safe to restart your aiccu now","date":"2008-09-08T00:00:00.000Z","formattedDate":"September 8, 2008","tags":[],"readingTime":0.455,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"sixxs-down","title":"SiXXs down?"},"unlisted":false,"prevItem":{"title":"Disable SSLv2 in Webmin","permalink":"/blog/disable-sslv2-in-webmin"},"nextItem":{"title":"iPhone 3G and Linksys WAG54G v3 incompatibility","permalink":"/blog/iphone-3g-and-linksys-wag54g-v3-incompatibility"}},"content":"<p>For some reason Sixxs.net seems to be down. I\'m unable to connect my IPV6 tunnels to their PoP\'s or connect to their website. Does anyone have any idea why they\'re offline? Update: Seems to be back up now. It\'s safe to restart your aiccu now</p>\\n\\n\\n\x3c!--truncate--\x3e\\nFor some reason Sixxs.net seems to be down. I\'m unable to connect my IPV6 tunnels to their PoP\'s or connect to their website. Does anyone have any idea why they\'re offline?  \\n\\nUpdate: Seems to be back up now. It\'s safe to restart your aiccu now"},{"id":"iphone-3g-and-linksys-wag54g-v3-incompatibility","metadata":{"permalink":"/blog/iphone-3g-and-linksys-wag54g-v3-incompatibility","source":"@site/blog/2008-07-12-iphone-3g-and-linksys-wag54g-v3-incompatibility.mdx","title":"iPhone 3G and Linksys WAG54G v3 incompatibility","description":"For some reason the iPhone 3G will not connect to this wireless router. There has been talk that this is fixed in the latest firmware but as of writing this it is not fixed in the latest AU/NZ firmware (1.00.23). I found a temporary solution for the problem though. If you limit your router to [&hellip;]","date":"2008-07-12T00:00:00.000Z","formattedDate":"July 12, 2008","tags":[],"readingTime":0.86,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"iphone-3g-and-linksys-wag54g-v3-incompatibility","title":"iPhone 3G and Linksys WAG54G v3 incompatibility"},"unlisted":false,"prevItem":{"title":"SiXXs down?","permalink":"/blog/sixxs-down"},"nextItem":{"title":"New postcodes compulsory for bulk mailers","permalink":"/blog/new-postcodes-compulsory-for-bulk-mailers"}},"content":"<p>For some reason the iPhone 3G will not connect to this wireless router. There has been talk that this is fixed in the latest firmware but as of writing this it is not fixed in the latest AU/NZ firmware (1.00.23). I found a temporary solution for the problem though. If you limit your router to [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nFor some reason the iPhone 3G will not connect to this wireless router. There has been talk that this is fixed in the latest firmware but as of writing this it is not fixed in the latest AU/NZ firmware (1.00.23).  \\n\\nI found a temporary solution for the problem though. If you limit your router to 802.11B only then the iPhone will connect without a problem. As soon as it\'s changed to mixed network or 802.11G only then it will fail to connect.  \\n\\nUpdate: I\'ve found that the AU firmware from the <a href=\\"http://www-au.linksys.com/servlet/Satellite?c=L_CASupport_C2&#038;childpagename=AU%2FLayout&#038;cid=1175234126907&#038;pagename=Linksys%2FCommon%2FVisitorWrapper&#038;lid=2690714626B148&#038;displaypage=download\\">AU linksys site</a> works fine on the NZ WAG54Gv3. I\'m using 1.00.46 and the iPhone 3G now connects under 802.11G without a problem."},{"id":"new-postcodes-compulsory-for-bulk-mailers","metadata":{"permalink":"/blog/new-postcodes-compulsory-for-bulk-mailers","source":"@site/blog/2008-07-02-new-postcodes-compulsory-for-bulk-mailers.mdx","title":"New postcodes compulsory for bulk mailers","description":"Tv3 reported on this the other night, but I can\'t find a link to the video clip. Apparently from July 1st, 2008 postcodes must be used when sending bulk mail to receive discounts. This is to force bulk mailers to move over to the new system which can be read easier by NZ Post. According [&hellip;]","date":"2008-07-02T00:00:00.000Z","formattedDate":"July 2, 2008","tags":[],"readingTime":0.87,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"new-postcodes-compulsory-for-bulk-mailers","title":"New postcodes compulsory for bulk mailers"},"unlisted":false,"prevItem":{"title":"iPhone 3G and Linksys WAG54G v3 incompatibility","permalink":"/blog/iphone-3g-and-linksys-wag54g-v3-incompatibility"},"nextItem":{"title":"DNS Stuff/DNS Report Alternative","permalink":"/blog/dns-stuffdns-report-alternative"}},"content":"<p>Tv3 reported on this the other night, but I can\'t find a link to the video clip. Apparently from July 1st, 2008 postcodes must be used when sending bulk mail to receive discounts. This is to force bulk mailers to move over to the new system which can be read easier by NZ Post. According [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nTv3 reported on this the other night, but I can\'t find a link to the video clip. Apparently from July 1st, 2008 postcodes must be used when sending bulk mail to receive discounts. This is to force bulk mailers to move over to the new system which can be read easier by NZ Post.  \\n\\nAccording to the news clip residentual mail may also be affected by this change over. Mail can be lost in the mail system for months if the wrong/old postcode is used. Examples were given where there are ~18 Beach Rd\'s in Auckland. Using the correct postcode allows mail sorters to send your letter to the right one.  \\n\\nMore information here: <a href=\\"http://www.nzherald.co.nz/section/1/story.cfm?c_id=1&amp;objectid=10519552\\" target=\\"_blank\\">http://www.nzherald.co.nz/section/1/story.cfm?c_id=1&amp;objectid=10519552</a>"},{"id":"dns-stuffdns-report-alternative","metadata":{"permalink":"/blog/dns-stuffdns-report-alternative","source":"@site/blog/2008-06-25-dns-stuffdns-report-alternative.mdx","title":"DNS Stuff/DNS Report Alternative","description":"Recently dnsstuff.com started charging for all their services and have doubled their prices. I know they\'re a business, but this move will force people away from their site onto other services. So I\'ve been looking for an alternative to dnsstuff.com for a while and came across this//dnssy.com/ It seems to have similar reports and [&hellip;]","date":"2008-06-25T00:00:00.000Z","formattedDate":"June 25, 2008","tags":[],"readingTime":0.61,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"dns-stuffdns-report-alternative","title":"DNS Stuff/DNS Report Alternative"},"unlisted":false,"prevItem":{"title":"New postcodes compulsory for bulk mailers","permalink":"/blog/new-postcodes-compulsory-for-bulk-mailers"},"nextItem":{"title":"Postcode Database V2 updated","permalink":"/blog/postcode-database-v2-updated"}},"content":"<p>Recently dnsstuff.com started charging for all their services and have doubled their prices. I know they\'re a business, but this move will force people away from their site onto other services. So I\'ve been looking for an alternative to dnsstuff.com for a while and came across this: http://dnssy.com/ It seems to have similar reports and [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nRecently dnsstuff.com started charging for all their services and have doubled their prices. I know they\'re a business, but this move will force people away from their site onto other services.  \\n\\nSo I\'ve been looking for an alternative to dnsstuff.com for a while and came across this: <a href=\\"http://dnssy.com/\\" target=\\"_blank\\">http://dnssy.com/</a><br />\\nIt seems to have similar reports and is good for debugging your DNS setup."},{"id":"postcode-database-v2-updated","metadata":{"permalink":"/blog/postcode-database-v2-updated","source":"@site/blog/2008-04-20-postcode-database-v2-updated.mdx","title":"Postcode Database V2 updated","description":"As a few people have pointed out in the comments there was an error in the initial release of the V2 postcode database, the suburb data had been lost along the way. I\'ve fixed the problem and updated the bzip2 and gzip files. I\'ve also removed the mssql version as it never worked anyway. I\'m [&hellip;]","date":"2008-04-20T00:00:00.000Z","formattedDate":"April 20, 2008","tags":[],"readingTime":0.66,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"postcode-database-v2-updated","title":"Postcode Database V2 updated"},"unlisted":false,"prevItem":{"title":"DNS Stuff/DNS Report Alternative","permalink":"/blog/dns-stuffdns-report-alternative"},"nextItem":{"title":"Optimized vBulletin (Part 2)","permalink":"/blog/optimized-vbulletin-part-2"}},"content":"<p>As a few people have pointed out in the comments there was an error in the initial release of the V2 postcode database, the suburb data had been lost along the way. I\'ve fixed the problem and updated the bzip2 and gzip files. I\'ve also removed the mssql version as it never worked anyway. I\'m [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nAs a few people have pointed out in the comments there was an error in the initial release of the V2 postcode database, the suburb data had been lost along the way.  \\n\\nI\'ve fixed the problem and updated the bzip2 and gzip files. I\'ve also removed the mssql version as it never worked anyway. I\'m not sure if the problem affects the csv version and I don\'t have time to look at it now.  \\n\\nEnjoy."},{"id":"optimized-vbulletin-part-2","metadata":{"permalink":"/blog/optimized-vbulletin-part-2","source":"@site/blog/2007-10-30-optimized-vbulletin-part-2.mdx","title":"Optimized vBulletin (Part 2)","description":"I\'m just trying out a new method to optimize vBulletin\'s front end scripts and CSS. It\'s still a work in progress but these are the results I have seen on my test site (www.wirelessforums.org) Without GZIP Base VB (v3.6.8) vbulletinglobal.js &#8211; 43.8KB vbulletinmenu.js &#8211; 17.8KB vbulletinreadmarker.js &#8211; 6.7KB vbulletin_md5.js &#8211; 9.6KB JS total &#8211; 77.9KB [&hellip;]","date":"2007-10-30T00:00:00.000Z","formattedDate":"October 30, 2007","tags":[],"readingTime":1.645,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"optimized-vbulletin-part-2","title":"Optimized vBulletin (Part 2)"},"unlisted":false,"prevItem":{"title":"Postcode Database V2 updated","permalink":"/blog/postcode-database-v2-updated"},"nextItem":{"title":"New Zealand Postcode Database (V2)","permalink":"/blog/new-zealand-postcode-database-v2"}},"content":"<p>I\'m just trying out a new method to optimize vBulletin\'s front end scripts and CSS. It\'s still a work in progress but these are the results I have seen on my test site (www.wirelessforums.org) Without GZIP Base VB (v3.6.8) vbulletin_global.js &#8211; 43.8KB vbulletin_menu.js &#8211; 17.8KB vbulletin_read_marker.js &#8211; 6.7KB vbulletin_md5.js &#8211; 9.6KB JS total &#8211; 77.9KB [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI\'m just trying out a new method to optimize vBulletin\'s front end scripts and CSS. It\'s still a work in progress but these are the results I have seen on my test site (www.wirelessforums.org)  \\n\\n<strong>Without GZIP</strong>  \\n\\nBase VB (v3.6.8)  \\n\\nvbulletin_global.js &#8211; 43.8KB<br />\\nvbulletin_menu.js &#8211; 17.8KB<br />\\nvbulletin_read_marker.js &#8211; 6.7KB<br />\\nvbulletin_md5.js &#8211; 9.6KB  \\n\\nJS total &#8211; 77.9KB  \\n\\ncss &#8211; 5.7KB  \\n\\nUsing minify  \\n\\nvbulletin_global.js &#8211; 23KB<br />\\nvbulletin_menu.js &#8211; 10.4KB<br />\\nvbulletin_read_marker.js &#8211; 3.3KB<br />\\nvbulletin_md5.js &#8211; 6KB  \\n\\nJS total &#8211; 42.7 (45.1% savings)  \\n\\ncss &#8211; 4.1K (28% savings)  \\n\\n<strong>With GZIP</strong>  \\n\\nBase VB (v3.6.8)  \\n\\nvbulletin_global.js &#8211; 12.5KB<br />\\nvbulletin_menu.js &#8211; 4.7KB<br />\\nvbulletin_read_marker.js &#8211; 2KB<br />\\nvbulletin_md5.js &#8211; 3.3KB  \\n\\nJS total &#8211; 22.5KB  \\n\\ncss &#8211; 1.3KB  \\n\\nUsing minify and GZIP  \\n\\nvbulletin_global.js &#8211; 6.9KB<br />\\nvbulletin_menu.js &#8211; 2.7KB<br />\\nvbulletin_read_marker.js &#8211; 1KB<br />\\nvbulletin_md5.js &#8211; 2.1KB  \\n\\nJS total &#8211; 12.7KB  \\n\\nThis is an extra 43.5% savings on top of GZIP. A total of <strong>65.2KB (83.7%) </strong>savings from the original.  \\n\\ncss &#8211; 1.1KB  \\n\\nAn extra 15.3% savings on top of GZIP. A total of <strong>4.6KB (80.7%) </strong>savings from the original.  \\n\\nThis is a huge improvement over the original file sizes and speeds up rendering of the page quite a bit. The new method doesn\'t require you to download JS replacements for each version of vB, instead it works off the existing files. Hopefully this will silence the people who complained about copyright/piracy issues with me distributing the JS files (even though they\'re distributed to every browser that downloads the page).  \\n\\nI\'m also looking at some other ways to speed up rendering as much as possible. Stay tuned for downloads and a full write up."},{"id":"new-zealand-postcode-database-v2","metadata":{"permalink":"/blog/new-zealand-postcode-database-v2","source":"@site/blog/2007-09-14-new-zealand-postcode-database-v2.mdx","title":"New Zealand Postcode Database (V2)","description":"I\'ve done a bit of work on the database, added a bit of information and corrected mistakes. Thanks to Owen and others for help with the data. If you find this data useful, please donate. What\'s new in V2 More accurate Added GPS co-ordinates Added PO Boxes and Private Bags Added Rural postcodes Removed regions [&hellip;]","date":"2007-09-14T00:00:00.000Z","formattedDate":"September 14, 2007","tags":[],"readingTime":1.43,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"new-zealand-postcode-database-v2","title":"New Zealand Postcode Database (V2)"},"unlisted":false,"prevItem":{"title":"Optimized vBulletin (Part 2)","permalink":"/blog/optimized-vbulletin-part-2"},"nextItem":{"title":"vBulletin JS Min","permalink":"/blog/vbulletin-js-min"}},"content":"<p>I\'ve done a bit of work on the database, added a bit of information and corrected mistakes. Thanks to Owen and others for help with the data. If you find this data useful, please donate. What\'s new in V2 More accurate Added GPS co-ordinates Added PO Boxes and Private Bags Added Rural postcodes Removed regions [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI\'ve done a bit of work on the database, added a bit of information and corrected mistakes.  \\n\\nThanks to Owen and others for help with the data.  \\n\\n*What\'s new in V2*\\n\\n- More accurate\\n- Added GPS co-ordinates\\n- Added PO Boxes and Private Bags\\n- Added Rural postcodes\\n- Removed regions (as some datasources labeled a city/town as one region and others differed.)\\n\\nI\'ve also taken the opportunity to include CSV and MSSQL formats, as well as MySQL.  \\n\\n*History* \\n\\nI originally created the database as I needed street and suburb data for a project I was doing. No one else was stupid enough to spend that much time on it, so I took it on. NZ Post still has their PAF data, but are unlikely to release a free database of the postcodes.  \\n\\n<a href=\\"http://www.noodles.net.nz/2006/08/09/nz-postcode-database/\\">Original Post</a>\\n<a rel=\\"nofollow\\" href=\\"http://en.wikipedia.org/wiki/New_Zealand_postal_addresses\\">More info on NZ Addresses</a>  \\n\\n*Downloads*\\n\\nMySQL\\n<a href=\\"http://www.noodles.net.nz/downloads/nzpostcodes_v2.sql.gz\\">nzpostcodes_v2.sql.gz</a> (655KB)\\n<a href=\\"http://www.noodles.net.nz/downloads/nzpostcodes_v2.sql.bz2\\">nzpostcodes_v2.sql.bz2</a> (445KB)  \\n\\nCSV\\n<a href=\\"http://www.noodles.net.nz/downloads/nzpostcodes_v2.csv.zip\\">nzpostcodes_v2.csv.zip</a> (1.07MB)  \\n\\nAll efforts have been made to ensure that there are no errors, but there are no guarantees, if you need 100% accurate data please buy NZ Post\u2019s PAF data.  \\n\\n[UPDATE 20/04/08]\\nAs a few people have pointed out, there was no suburb data in the MySQL database version of the postcode database. This has now been fixed. Sorry about the problems and how long it\'s taken me to get around to fixing it."},{"id":"vbulletin-js-min","metadata":{"permalink":"/blog/vbulletin-js-min","source":"@site/blog/2007-08-08-vbulletin-js-min.mdx","title":"vBulletin JS Min","description":"I realized today that vBulletin\'s Javascript was just too bulky and didn\'t really need to be, especially when you have no plans to customize it yourself. So I grabbed all the packages of 3.6.x and minimized all the Javascript. In most cases the Javascript files are 50% of the original, which saves bandwidth and speeds [&hellip;]","date":"2007-08-08T00:00:00.000Z","formattedDate":"August 8, 2007","tags":[],"readingTime":0.94,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"vbulletin-js-min","title":"vBulletin JS Min"},"unlisted":false,"prevItem":{"title":"New Zealand Postcode Database (V2)","permalink":"/blog/new-zealand-postcode-database-v2"},"nextItem":{"title":"Multiple versions of IE on one PC","permalink":"/blog/multiple-versions-of-ie-on-one-pc"}},"content":"<p>I realized today that vBulletin\'s Javascript was just too bulky and didn\'t really need to be, especially when you have no plans to customize it yourself. So I grabbed all the packages of 3.6.x and minimized all the Javascript. In most cases the Javascript files are 50% of the original, which saves bandwidth and speeds [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI realized today that vBulletin\'s Javascript was just too bulky and didn\'t really need to be, especially when you have no plans to customize it yourself. So I grabbed all the packages of 3.6.x and minimized all the Javascript. In most cases the Javascript files are 50% of the original, which saves bandwidth and speeds up the loading of your forum.  \\n\\nInstall:  \\n\\n- Backup all the .js files from the clientscript directory under your forum root.\\n- Copy the files from zip file that matches your version of vBulletin into the clientscript directory.\\n- Done!\\n\\nDownload:  \\n\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.0_min.zip\\">vBulletin 3.6.0</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.1_min.zip\\">vBulletin 3.6.1</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.2_min.zip\\">vBulletin 3.6.2</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.3_min.zip\\">vBulletin 3.6.3</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.4_min.zip\\">vBulletin 3.6.4</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.5_min.zip\\">vBulletin 3.6.5</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.6_min.zip\\">vBulletin 3.6.6</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.7PL1_min.zip\\">vBulletin 3.6.7PL1</a><br />\\n<a href=\\"http://www.noodles.net.nz/downloads/clientscript_3.6.8_min.zip\\">vBulletin 3.6.8</a>"},{"id":"multiple-versions-of-ie-on-one-pc","metadata":{"permalink":"/blog/multiple-versions-of-ie-on-one-pc","source":"@site/blog/2007-08-03-multiple-versions-of-ie-on-one-pc.mdx","title":"Multiple versions of IE on one PC","description":"I\'m always getting grief from my managers about not testing in IE6, which is hard when IE7 is installed on all machines these days. I recently found a little install tool which allows you to run multiple versions of IE on the same machine. It doesn\'t fully install IE, just the dll\'s and uses dll [&hellip;]","date":"2007-08-03T00:00:00.000Z","formattedDate":"August 3, 2007","tags":[],"readingTime":0.735,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"multiple-versions-of-ie-on-one-pc","title":"Multiple versions of IE on one PC"},"unlisted":false,"prevItem":{"title":"vBulletin JS Min","permalink":"/blog/vbulletin-js-min"},"nextItem":{"title":"Postcode Database Update Pending","permalink":"/blog/postcode-database-update-pending"}},"content":"<p>I\'m always getting grief from my managers about not testing in IE6, which is hard when IE7 is installed on all machines these days. I recently found a little install tool which allows you to run multiple versions of IE on the same machine. It doesn\'t fully install IE, just the dll\'s and uses dll [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI\'m always getting grief from my managers about not testing in IE6, which is hard when IE7 is installed on all machines these days. I recently found a little install tool which allows you to run multiple versions of IE on the same machine. It doesn\'t fully install IE, just the dll\'s and uses dll redirection to trick windows into running an older version of IE.  \\n\\nAlthough it\'s not perfect, it\'s a lot easier than running a virtual PC for each version.  \\n\\nThe install can be found here: <a href=\\"http://tredosoft.com/Multiple_IE\\">http://tredosoft.com/Multiple_IE</a>"},{"id":"postcode-database-update-pending","metadata":{"permalink":"/blog/postcode-database-update-pending","source":"@site/blog/2007-07-05-postcode-database-update-pending.mdx","title":"Postcode Database Update Pending","description":"I\'ve just received some data from Owen for RD and PO Box postcodes, so I\'ll be added these into the database. Also, stay tuned for Long/Lat coordinates for streets and postcodes.","date":"2007-07-05T00:00:00.000Z","formattedDate":"July 5, 2007","tags":[],"readingTime":0.315,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"postcode-database-update-pending","title":"Postcode Database Update Pending"},"unlisted":false,"prevItem":{"title":"Multiple versions of IE on one PC","permalink":"/blog/multiple-versions-of-ie-on-one-pc"},"nextItem":{"title":"Monitoring Apache with LARRD and Big Brother","permalink":"/blog/monitoring-apache-with-larrd-and-big-brother"}},"content":"<p>I\'ve just received some data from Owen for RD and PO Box postcodes, so I\'ll be added these into the database. Also, stay tuned for Long/Lat coordinates for streets and postcodes.</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI\'ve just received some data from Owen for RD and PO Box postcodes, so I\'ll be added these into the database.  \\n\\nAlso, stay tuned for Long/Lat coordinates for streets and postcodes."},{"id":"monitoring-apache-with-larrd-and-big-brother","metadata":{"permalink":"/blog/monitoring-apache-with-larrd-and-big-brother","source":"@site/blog/2007-05-25-monitoring-apache-with-larrd-and-big-brother.mdx","title":"Monitoring Apache with LARRD and Big Brother","description":"I managed to figure out how to monitor Apache with Big Brother and LARRD. As it\'s not very well documented I thought I\'d share it with everyone. First set your apache server to display status in the apache httpd.conf: [code] SetHandler server-status Order deny,allow Deny from all Allow from 127.0.0.1 ExtendedStatus On [/code] Setting extended [&hellip;]","date":"2007-05-25T00:00:00.000Z","formattedDate":"May 25, 2007","tags":[],"readingTime":1.355,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"monitoring-apache-with-larrd-and-big-brother","title":"Monitoring Apache with LARRD and Big Brother"},"unlisted":false,"prevItem":{"title":"Postcode Database Update Pending","permalink":"/blog/postcode-database-update-pending"},"nextItem":{"title":"Google Sitemap Generator","permalink":"/blog/2007/01/31/google-sitemap-generator"}},"content":"<p>I managed to figure out how to monitor Apache with Big Brother and LARRD. As it\'s not very well documented I thought I\'d share it with everyone. First set your apache server to display status in the apache httpd.conf: [code] SetHandler server-status Order deny,allow Deny from all Allow from 127.0.0.1 ExtendedStatus On [/code] Setting extended [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI managed to figure out how to monitor Apache with Big Brother and LARRD. As it\'s not very well documented I thought I\'d share it with everyone.  \\n\\nFirst set your apache server to display status in the apache httpd.conf:  \\n\\n```\\n<Location /server-status>\\nSetHandler server-status\\nOrder deny,allow\\nDeny from all\\nAllow from 127.0.0.1\\n</Location>  \\n\\nExtendedStatus On\\n```\\n\\nSetting extended status to On does slow down the server a little bit, but it\'s usually not noticeable. Remember to limit access to the status page to just the IP where your Big Brother server is monitoring from.  \\n\\nAdd apache to @DATALIST in larrd-config.pl  \\n\\nThen in your apache-larrd.pl (under the larrd directory) you\'ll need to hardcode the servers you want to monitor.  \\n\\n```\\n%host_h = (\\n    \u2018server01\u2019 => {\\n        port => \u201c80\u201d,\\n        url => \u201chttp://www.yoursite.com/server-status?auto\u201d,\\n    }\\n);\\n``` \\n\\nThe first part of the host_h array is to identify which server to attach the RRD file with. In this instance I\'m monitoring a server called server01 and the website I have setup with the server-status page is http://www.youresite.com/server-status?auto. If your host is a FQDN (fully qualified domain name) then you can omit the full url and just have /server-status?auto and Big Brother will work out the URL.  \\n\\nWith any luck you\'ll have 4 new graphs on your trends/LARRD page."},{"id":"/2007/01/31/google-sitemap-generator","metadata":{"permalink":"/blog/2007/01/31/google-sitemap-generator","source":"@site/blog/2007-01-31-google-sitemap-generator.mdx","title":"Google Sitemap Generator","description":"I was dissapointed when I saw the current Google sitemap generators available, so I wrote one. [&hellip;]","date":"2007-01-31T00:00:00.000Z","formattedDate":"January 31, 2007","tags":[],"readingTime":0.82,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Google Sitemap Generator"},"unlisted":false,"prevItem":{"title":"Monitoring Apache with LARRD and Big Brother","permalink":"/blog/monitoring-apache-with-larrd-and-big-brother"},"nextItem":{"title":"MySQL and Redhat Enterprise Linux 4","permalink":"/blog/2006/12/01/mysql-and-redhat-enterprise-linux-4"}},"content":"<p>I was dissapointed when I saw the current Google sitemap generators available, so I wrote one. [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI was dissapointed when I saw the current Google sitemap generators available, so I wrote one.  \\n\\nFeatures:\\n- Generates Google sitemaps\\n- Generates Yahoo!  sitemaps\\n- Generates plain text sitemap\\n- Splits sitemaps up into smaller files (as per Google\'s instructions)\\n- Creates sitemap index file\\n- Pings Google/Yahoo!  \\n\\nThis is the first release, so there\'s probably some bugs in there somewhere. Please let me know if you find any.  There are some examples in the zip file.  \\n\\nTo best use the classes, set the variables on the page you call the class from, rather than editing the class file (so changes aren\'t lost when you upgrade).  \\n\\nE.g.  \\n\\n```\\n$sitemap = new Google_Sitemap();\\n$sitemap->doPing = false;\\n$sitemap->gzip = false;\\n$sitemap->baseurl = \\"https://www.noodles.net.nz\\";\\n```\\n\\nDownload:  \\n\\n[sitemaps.zip](/downloads/sitemaps.zip) \\n\\nVersion 0.2 - 14/05/2007  \\n\\nFixed problem with script using too much memory when trying to sitemap hundreds of thousands of pages."},{"id":"/2006/12/01/mysql-and-redhat-enterprise-linux-4","metadata":{"permalink":"/blog/2006/12/01/mysql-and-redhat-enterprise-linux-4","source":"@site/blog/2006-12-01-mysql-and-redhat-enterprise-linux-4.mdx","title":"MySQL and Redhat Enterprise Linux 4","description":"I ran into some problems with MySQL on RHEL 4 when SELinux was enabled. When starting MySQL it would come up with this [&hellip;]","date":"2006-12-01T00:00:00.000Z","formattedDate":"December 1, 2006","tags":[],"readingTime":0.435,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"MySQL and Redhat Enterprise Linux 4"},"unlisted":false,"prevItem":{"title":"Google Sitemap Generator","permalink":"/blog/2007/01/31/google-sitemap-generator"},"nextItem":{"title":"Double byte and PHP (unicode problems)","permalink":"/blog/2006/11/02/double-byte-and-php-unicode-problems"}},"content":"<p>I ran into some problems with MySQL on RHEL 4 when SELinux was enabled. When starting MySQL it would come up with this [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nI ran into some problems with MySQL on RHEL 4 when SELinux was enabled. When starting MySQL it would come up with this:  \\n\\n```\\nStarting MySQL...................................[FAILED]\\n```\\nAnd the following in /var/log/messages:  \\n\\n```\\nDec  1 14:02:52 searchdev kernel: audit(1164934972.432:5): avc:  denied  { append } for  pid=3071 comm=\\"mysqld\\" name=\\"searchdev.err\\" dev=sda6 ino=1687755 scontext=root:system_r:mysqld_t tcontext=root:object_r:var_lib_t tclass=file[/code]  \\n```\\n\\nAll fixed though thanks to this page: [https://bugs.mysql.com/bug.php?id=12676](https://bugs.mysql.com/bug.php?id=12676)"},{"id":"/2006/11/02/double-byte-and-php-unicode-problems","metadata":{"permalink":"/blog/2006/11/02/double-byte-and-php-unicode-problems","source":"@site/blog/2006-11-02-double-byte-and-php-unicode-problems.mdx","title":"Double byte and PHP (unicode problems)","description":"A while back I ran into a problem with PHP, how can I read in files that have double byte (unicode) characters and display them in a form that any browser can read. Most programming languages don\'t handle these characters very well, and end up with non sense instead of passing through the correct text. [&hellip;]","date":"2006-11-02T00:00:00.000Z","formattedDate":"November 2, 2006","tags":[],"readingTime":0.81,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Double byte and PHP (unicode problems)"},"unlisted":false,"prevItem":{"title":"MySQL and Redhat Enterprise Linux 4","permalink":"/blog/2006/12/01/mysql-and-redhat-enterprise-linux-4"},"nextItem":{"title":"Google datacenter check","permalink":"/blog/2006/11/02/google-datacenter-check"}},"content":"<p>A while back I ran into a problem with PHP, how can I read in files that have double byte (unicode) characters and display them in a form that any browser can read. Most programming languages don\'t handle these characters very well, and end up with non sense instead of passing through the correct text. [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nA while back I ran into a problem with PHP, how can I read in files that have double byte (unicode) characters and display them in a form that any browser can read. Most programming languages don\'t handle these characters very well, and end up with non sense instead of passing through the correct text.  \\n\\nThis function should be able to strip out any unicode characters from text and return them as html entities (something none of the core PHP functions are able to do).  \\n\\n```\\nfunction removeuni($content){\\npreg_match_all(\\"/[\\\\x{90}-\\\\x{3000}]/u\\", $content, $matches);  \\n\\nforeach($matches[0] as $match){\\n$content = str_replace($match, mb_convert_encoding($match, \\"HTML-ENTITIES\\",\\"UTF-8&#8243;), $content);\\n}  \\n\\nreturn $content;\\n}\\n```"},{"id":"/2006/11/02/google-datacenter-check","metadata":{"permalink":"/blog/2006/11/02/google-datacenter-check","source":"@site/blog/2006-11-02-google-datacenter-check.mdx","title":"Google datacenter check","description":"I\'ve written a crude datacenter checker. Unfortunately I cannot host it for everyone as Google would ban my server\'s IP, so I\'ve packaged up the code so anyone can run it. [&hellip;]","date":"2006-11-02T00:00:00.000Z","formattedDate":"November 2, 2006","tags":[],"readingTime":0.325,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Google datacenter check"},"unlisted":false,"prevItem":{"title":"Double byte and PHP (unicode problems)","permalink":"/blog/2006/11/02/double-byte-and-php-unicode-problems"},"nextItem":{"title":"New Zealand Postcode Database","permalink":"/blog/2006/08/09/nz-postcode-database"}},"content":"I\'ve written a crude datacenter checker. Unfortunately I cannot host it for everyone as Google would ban my server\'s IP, so I\'ve packaged up the code so anyone can run it. [&hellip;]\\n\\n\\n\x3c!--truncate--\x3e\\nI\'ve written a crude datacenter checker. Unfortunately I cannot host it for everyone as Google would ban my server\'s IP, so I\'ve packaged up the code so anyone can run it.  \\n\\n[googlecheck.zip](/downloads/googlecheck.zip)"},{"id":"/2006/08/09/nz-postcode-database","metadata":{"permalink":"/blog/2006/08/09/nz-postcode-database","source":"@site/blog/2006-08-09-nz-postcode-database.mdx","title":"New Zealand Postcode Database","description":"Due to the lack of data from NZ Post, I\'ve created this mysql database from the PDF on the NZ Post website. All efforts have been made to ensure that there are no errors, but there are no guarantees, if you need 100% accurate data please buy NZ Post\'s PAF data. Update: New Zealand Postcode [&hellip;]","date":"2006-08-09T00:00:00.000Z","formattedDate":"August 9, 2006","tags":[],"readingTime":0.605,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"New Zealand Postcode Database"},"unlisted":false,"prevItem":{"title":"Google datacenter check","permalink":"/blog/2006/11/02/google-datacenter-check"}},"content":"<p>Due to the lack of data from NZ Post, I\'ve created this mysql database from the PDF on the NZ Post website. All efforts have been made to ensure that there are no errors, but there are no guarantees, if you need 100% accurate data please buy NZ Post\'s PAF data. Update: New Zealand Postcode [&hellip;]</p>\\n\\n\\n\x3c!--truncate--\x3e\\nDue to the lack of data from NZ Post, I\'ve created this mysql database from the PDF on the NZ Post website.  \\n\\nAll efforts have been made to ensure that there are no errors, but there are no guarantees, if you need 100% accurate data please buy NZ Post\'s PAF data.  \\n\\nUpdate: [New Zealand Postcode Database version 2](2007-09-14-new-zealand-postcode-database-v2.mdx)\\n\\nVersion 1:  \\n\\n[nzpostcodes.sql.gz](/downloads/nzpostcodes.sql.gz) (467KB)\\n[nzpostcodes.sql.bz2](/downloads/nzpostcodes.sql.bz2) (313KB)"}]}')}}]);